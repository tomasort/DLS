saving to /scratch/tor213/DLS/question4/Hoffer/results/2022-03-31_05-06-17_resnet44_m-1
creating model resnet
created model with configuration: {'dataset': 'cifar10'}
number of parameters: 661338
/scratch/tor213/.env/lib64/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
optimization regime: [{'epoch': 0, 'optimizer': 'SGD', 'lr': 0.1, 'momentum': 0.9, 'regularizer': {'name': 'WeightDecay', 'value': 0.0001, 'log': False, 'filter': {'parameter_name': <function weight_decay_config.<locals>.<lambda> at 0x14600e53b940>, 'module': <function weight_decay_config.<locals>.<lambda> at 0x14600e53ba60>}}}, {'epoch': 81, 'lr': 0.01}, {'epoch': 122, 'lr': 0.001}, {'epoch': 164, 'lr': 0.0001}]
data regime: Current: {'datasets_path': '~/Datasets', 'name': 'cifar10', 'split': 'train', 'augment': True, 'input_size': None, 'batch_size': 512, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'drop_last': True, 'distributed': False, 'duplicates': 1, 'autoaugment': False, 'cutout': None}
 Regime:None

Starting Epoch: 1

Files already downloaded and verified
Files already downloaded and verified
TRAINING - Epoch: [0][0/97]	Time 2.182 (2.182)	Data 1.737 (1.737)	Loss 2.3050 (2.3050)	Prec@1 9.766 (9.766)	Prec@5 48.633 (48.633)	Acc 0.098 (0.098)	
TRAINING - Epoch: [0][10/97]	Time 0.072 (0.266)	Data 0.000 (0.158)	Loss 2.2193 (2.2733)	Prec@1 19.727 (15.874)	Prec@5 70.703 (60.263)	Acc 0.159 (0.141)	
TRAINING - Epoch: [0][20/97]	Time 0.072 (0.174)	Data 0.000 (0.083)	Loss 2.0908 (2.2110)	Prec@1 24.219 (18.220)	Prec@5 73.047 (65.811)	Acc 0.182 (0.156)	
TRAINING - Epoch: [0][30/97]	Time 0.072 (0.141)	Data 0.000 (0.056)	Loss 1.9571 (2.1475)	Prec@1 25.000 (20.331)	Prec@5 81.445 (69.790)	Acc 0.203 (0.169)	
TRAINING - Epoch: [0][40/97]	Time 0.072 (0.124)	Data 0.000 (0.043)	Loss 1.9751 (2.1054)	Prec@1 25.195 (21.784)	Prec@5 80.469 (72.256)	Acc 0.218 (0.179)	
TRAINING - Epoch: [0][50/97]	Time 0.073 (0.114)	Data 0.000 (0.034)	Loss 1.8691 (2.0669)	Prec@1 27.734 (22.986)	Prec@5 84.375 (74.265)	Acc 0.230 (0.188)	
TRAINING - Epoch: [0][60/97]	Time 0.072 (0.107)	Data 0.000 (0.029)	Loss 1.8490 (2.0316)	Prec@1 28.906 (24.123)	Prec@5 84.180 (75.884)	Acc 0.241 (0.196)	
TRAINING - Epoch: [0][70/97]	Time 0.072 (0.102)	Data 0.000 (0.025)	Loss 1.7974 (2.0023)	Prec@1 31.445 (24.986)	Prec@5 86.914 (77.236)	Acc 0.250 (0.203)	
TRAINING - Epoch: [0][80/97]	Time 0.072 (0.099)	Data 0.000 (0.022)	Loss 1.7190 (1.9733)	Prec@1 36.719 (25.803)	Prec@5 88.867 (78.441)	Acc 0.258 (0.209)	
TRAINING - Epoch: [0][90/97]	Time 0.072 (0.096)	Data 0.000 (0.019)	Loss 1.7530 (1.9476)	Prec@1 28.320 (26.552)	Prec@5 88.086 (79.436)	Acc 0.266 (0.215)	
TRAINING - Epoch: [0][96/97]	Time 0.072 (0.094)	Data 0.000 (0.018)	Loss 1.7297 (1.9339)	Prec@1 34.766 (26.923)	Prec@5 87.109 (79.981)	Acc 0.269 (0.218)	
Traceback (most recent call last):
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 364, in <module>
    main()
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 130, in main
    main_worker(args)
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 307, in main_worker
    train_results = trainer.train(train_data.get_loader(), chunk_batch=args.chunk_batch)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 282, in train
    return self.forward(data_loader, training=True, average_output=average_output, chunk_batch=chunk_batch)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 276, in forward
    return meter_results(meters)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 193, in meter_results
    results = {name: meter.avg for name, meter in meters.items()}
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 193, in <dictcomp>
    results = {name: meter.avg for name, meter in meters.items()}
AttributeError: 'float' object has no attribute 'avg'
saving to /scratch/tor213/DLS/question4/Hoffer/results/2022-03-31_05-06-33_resnet44_m-2
creating model resnet
created model with configuration: {'dataset': 'cifar10', 'depth': 44}
number of parameters: 661338
/scratch/tor213/.env/lib64/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
optimization regime: [{'epoch': 0, 'optimizer': 'SGD', 'lr': 0.1, 'momentum': 0.9, 'regularizer': {'name': 'WeightDecay', 'value': 0.0001, 'log': False, 'filter': {'parameter_name': <function weight_decay_config.<locals>.<lambda> at 0x1479879ebaf0>, 'module': <function weight_decay_config.<locals>.<lambda> at 0x1479879eb940>}}}, {'epoch': 81, 'lr': 0.01}, {'epoch': 122, 'lr': 0.001}, {'epoch': 164, 'lr': 0.0001}]
data regime: Current: {'datasets_path': '~/Datasets', 'name': 'cifar10', 'split': 'train', 'augment': True, 'input_size': None, 'batch_size': 64, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'drop_last': True, 'distributed': False, 'duplicates': 2, 'autoaugment': False, 'cutout': {'holes': 1, 'length': 16}}
 Regime:None

Starting Epoch: 1

Files already downloaded and verified
Files already downloaded and verified
TRAINING - Epoch: [0][0/781]	Time 1.759 (1.759)	Data 1.602 (1.602)	Loss 2.3067 (2.3067)	Prec@1 7.031 (7.031)	Prec@5 42.188 (42.188)	Acc 0.070 (0.070)	
TRAINING - Epoch: [0][10/781]	Time 0.023 (0.182)	Data 0.000 (0.146)	Loss 2.3237 (2.3041)	Prec@1 9.375 (12.855)	Prec@5 56.250 (55.398)	Acc 0.129 (0.112)	
TRAINING - Epoch: [0][20/781]	Time 0.023 (0.107)	Data 0.000 (0.076)	Loss 2.2553 (2.2598)	Prec@1 10.938 (14.918)	Prec@5 64.844 (59.524)	Acc 0.149 (0.127)	
TRAINING - Epoch: [0][30/781]	Time 0.023 (0.080)	Data 0.000 (0.052)	Loss 2.1427 (2.2288)	Prec@1 25.781 (16.431)	Prec@5 69.531 (62.651)	Acc 0.164 (0.137)	
TRAINING - Epoch: [0][40/781]	Time 0.023 (0.067)	Data 0.000 (0.039)	Loss 2.2123 (2.1882)	Prec@1 17.969 (17.492)	Prec@5 73.438 (66.101)	Acc 0.175 (0.145)	
TRAINING - Epoch: [0][50/781]	Time 0.024 (0.059)	Data 0.000 (0.032)	Loss 1.9027 (2.1590)	Prec@1 27.344 (18.444)	Prec@5 84.375 (68.413)	Acc 0.184 (0.152)	
TRAINING - Epoch: [0][60/781]	Time 0.023 (0.053)	Data 0.000 (0.026)	Loss 1.8863 (2.1330)	Prec@1 22.656 (19.032)	Prec@5 85.156 (69.941)	Acc 0.190 (0.158)	
TRAINING - Epoch: [0][70/781]	Time 0.023 (0.049)	Data 0.000 (0.023)	Loss 2.0052 (2.1204)	Prec@1 28.906 (19.399)	Prec@5 82.031 (71.006)	Acc 0.194 (0.163)	
TRAINING - Epoch: [0][80/781]	Time 0.023 (0.046)	Data 0.000 (0.020)	Loss 2.0240 (2.1071)	Prec@1 19.531 (19.840)	Prec@5 76.562 (71.798)	Acc 0.198 (0.167)	
TRAINING - Epoch: [0][90/781]	Time 0.024 (0.044)	Data 0.000 (0.018)	Loss 2.0108 (2.0911)	Prec@1 22.656 (20.493)	Prec@5 71.875 (72.725)	Acc 0.205 (0.171)	
TRAINING - Epoch: [0][100/781]	Time 0.023 (0.042)	Data 0.000 (0.016)	Loss 1.9281 (2.0756)	Prec@1 32.812 (21.326)	Prec@5 78.906 (73.554)	Acc 0.213 (0.175)	
TRAINING - Epoch: [0][110/781]	Time 0.023 (0.040)	Data 0.000 (0.015)	Loss 1.8245 (2.0633)	Prec@1 32.031 (21.903)	Prec@5 85.938 (74.282)	Acc 0.219 (0.178)	
TRAINING - Epoch: [0][120/781]	Time 0.035 (0.039)	Data 0.000 (0.013)	Loss 1.8898 (2.0504)	Prec@1 26.562 (22.262)	Prec@5 84.375 (74.942)	Acc 0.223 (0.182)	
TRAINING - Epoch: [0][130/781]	Time 0.030 (0.038)	Data 0.000 (0.012)	Loss 1.9401 (2.0366)	Prec@1 29.688 (22.734)	Prec@5 79.688 (75.567)	Acc 0.227 (0.185)	
TRAINING - Epoch: [0][140/781]	Time 0.023 (0.037)	Data 0.000 (0.012)	Loss 1.8563 (2.0271)	Prec@1 25.781 (22.989)	Prec@5 87.500 (76.141)	Acc 0.230 (0.188)	
TRAINING - Epoch: [0][150/781]	Time 0.031 (0.036)	Data 0.000 (0.011)	Loss 1.8570 (2.0205)	Prec@1 24.219 (23.355)	Prec@5 85.938 (76.542)	Acc 0.234 (0.191)	
TRAINING - Epoch: [0][160/781]	Time 0.023 (0.036)	Data 0.000 (0.010)	Loss 2.0019 (2.0113)	Prec@1 28.906 (23.656)	Prec@5 78.125 (76.975)	Acc 0.237 (0.194)	
TRAINING - Epoch: [0][170/781]	Time 0.023 (0.035)	Data 0.000 (0.010)	Loss 1.8869 (2.0029)	Prec@1 23.438 (23.858)	Prec@5 78.906 (77.472)	Acc 0.239 (0.197)	
TRAINING - Epoch: [0][180/781]	Time 0.023 (0.034)	Data 0.000 (0.009)	Loss 1.7910 (1.9922)	Prec@1 30.469 (24.197)	Prec@5 84.375 (77.870)	Acc 0.242 (0.199)	
TRAINING - Epoch: [0][190/781]	Time 0.023 (0.034)	Data 0.000 (0.009)	Loss 1.7783 (1.9834)	Prec@1 37.500 (24.526)	Prec@5 86.719 (78.244)	Acc 0.245 (0.201)	
TRAINING - Epoch: [0][200/781]	Time 0.023 (0.033)	Data 0.000 (0.008)	Loss 1.8788 (1.9745)	Prec@1 25.781 (24.724)	Prec@5 83.594 (78.646)	Acc 0.247 (0.204)	
TRAINING - Epoch: [0][210/781]	Time 0.023 (0.033)	Data 0.000 (0.008)	Loss 1.7281 (1.9662)	Prec@1 32.812 (24.989)	Prec@5 89.844 (79.051)	Acc 0.250 (0.206)	
TRAINING - Epoch: [0][220/781]	Time 0.023 (0.033)	Data 0.000 (0.007)	Loss 1.9644 (1.9591)	Prec@1 32.812 (25.424)	Prec@5 78.906 (79.327)	Acc 0.254 (0.208)	
TRAINING - Epoch: [0][230/781]	Time 0.023 (0.032)	Data 0.000 (0.007)	Loss 1.8547 (1.9525)	Prec@1 28.906 (25.693)	Prec@5 84.375 (79.603)	Acc 0.257 (0.210)	
TRAINING - Epoch: [0][240/781]	Time 0.023 (0.032)	Data 0.000 (0.007)	Loss 1.6835 (1.9440)	Prec@1 38.281 (26.041)	Prec@5 86.719 (79.931)	Acc 0.260 (0.212)	
TRAINING - Epoch: [0][250/781]	Time 0.024 (0.032)	Data 0.000 (0.007)	Loss 1.7178 (1.9362)	Prec@1 39.062 (26.292)	Prec@5 88.281 (80.257)	Acc 0.263 (0.214)	
TRAINING - Epoch: [0][260/781]	Time 0.023 (0.031)	Data 0.000 (0.006)	Loss 1.9371 (1.9309)	Prec@1 33.594 (26.503)	Prec@5 76.562 (80.463)	Acc 0.265 (0.216)	
TRAINING - Epoch: [0][270/781]	Time 0.032 (0.031)	Data 0.000 (0.006)	Loss 1.8179 (1.9254)	Prec@1 32.812 (26.678)	Prec@5 78.125 (80.717)	Acc 0.267 (0.218)	
TRAINING - Epoch: [0][280/781]	Time 0.023 (0.031)	Data 0.000 (0.006)	Loss 1.5632 (1.9166)	Prec@1 46.875 (26.971)	Prec@5 92.188 (81.008)	Acc 0.270 (0.219)	
TRAINING - Epoch: [0][290/781]	Time 0.035 (0.031)	Data 0.000 (0.006)	Loss 1.8225 (1.9100)	Prec@1 31.250 (27.239)	Prec@5 89.062 (81.271)	Acc 0.272 (0.221)	
TRAINING - Epoch: [0][300/781]	Time 0.023 (0.031)	Data 0.000 (0.005)	Loss 1.5040 (1.9021)	Prec@1 42.188 (27.512)	Prec@5 89.844 (81.478)	Acc 0.275 (0.223)	
TRAINING - Epoch: [0][310/781]	Time 0.023 (0.031)	Data 0.000 (0.005)	Loss 1.6680 (1.8970)	Prec@1 36.719 (27.793)	Prec@5 89.062 (81.567)	Acc 0.278 (0.225)	
TRAINING - Epoch: [0][320/781]	Time 0.030 (0.030)	Data 0.000 (0.005)	Loss 1.7934 (1.8921)	Prec@1 31.250 (27.964)	Prec@5 79.688 (81.720)	Acc 0.280 (0.226)	
TRAINING - Epoch: [0][330/781]	Time 0.024 (0.030)	Data 0.000 (0.005)	Loss 1.7398 (1.8862)	Prec@1 39.062 (28.201)	Prec@5 87.500 (81.906)	Acc 0.282 (0.228)	
TRAINING - Epoch: [0][340/781]	Time 0.024 (0.030)	Data 0.000 (0.005)	Loss 1.8170 (1.8803)	Prec@1 36.719 (28.459)	Prec@5 85.938 (82.077)	Acc 0.285 (0.230)	
TRAINING - Epoch: [0][350/781]	Time 0.023 (0.030)	Data 0.000 (0.005)	Loss 1.8236 (1.8739)	Prec@1 27.344 (28.722)	Prec@5 85.156 (82.243)	Acc 0.287 (0.231)	
TRAINING - Epoch: [0][360/781]	Time 0.023 (0.030)	Data 0.000 (0.005)	Loss 1.5356 (1.8676)	Prec@1 39.844 (29.032)	Prec@5 90.625 (82.445)	Acc 0.290 (0.233)	
TRAINING - Epoch: [0][370/781]	Time 0.023 (0.030)	Data 0.000 (0.004)	Loss 1.6274 (1.8621)	Prec@1 41.406 (29.252)	Prec@5 88.281 (82.634)	Acc 0.293 (0.234)	
TRAINING - Epoch: [0][380/781]	Time 0.023 (0.029)	Data 0.000 (0.004)	Loss 1.5058 (1.8558)	Prec@1 44.531 (29.503)	Prec@5 92.969 (82.798)	Acc 0.295 (0.236)	
TRAINING - Epoch: [0][390/781]	Time 0.030 (0.029)	Data 0.000 (0.004)	Loss 1.7251 (1.8503)	Prec@1 36.719 (29.753)	Prec@5 89.062 (82.962)	Acc 0.298 (0.238)	
TRAINING - Epoch: [0][400/781]	Time 0.023 (0.029)	Data 0.000 (0.004)	Loss 1.5970 (1.8458)	Prec@1 35.938 (29.954)	Prec@5 92.188 (83.105)	Acc 0.300 (0.239)	
TRAINING - Epoch: [0][410/781]	Time 0.024 (0.029)	Data 0.000 (0.004)	Loss 1.6916 (1.8408)	Prec@1 34.375 (30.159)	Prec@5 89.062 (83.248)	Acc 0.302 (0.241)	
TRAINING - Epoch: [0][420/781]	Time 0.023 (0.029)	Data 0.000 (0.004)	Loss 1.6597 (1.8368)	Prec@1 35.156 (30.324)	Prec@5 92.969 (83.380)	Acc 0.303 (0.242)	
TRAINING - Epoch: [0][430/781]	Time 0.023 (0.029)	Data 0.000 (0.004)	Loss 1.6188 (1.8318)	Prec@1 34.375 (30.538)	Prec@5 92.969 (83.505)	Acc 0.305 (0.244)	
TRAINING - Epoch: [0][440/781]	Time 0.023 (0.029)	Data 0.000 (0.004)	Loss 1.6330 (1.8262)	Prec@1 40.625 (30.796)	Prec@5 88.281 (83.650)	Acc 0.308 (0.245)	
TRAINING - Epoch: [0][450/781]	Time 0.024 (0.029)	Data 0.000 (0.004)	Loss 1.5921 (1.8213)	Prec@1 36.719 (30.994)	Prec@5 86.719 (83.795)	Acc 0.310 (0.246)	
TRAINING - Epoch: [0][460/781]	Time 0.023 (0.029)	Data 0.000 (0.004)	Loss 1.6798 (1.8168)	Prec@1 33.594 (31.218)	Prec@5 91.406 (83.928)	Acc 0.312 (0.248)	
TRAINING - Epoch: [0][470/781]	Time 0.029 (0.028)	Data 0.000 (0.004)	Loss 1.6680 (1.8106)	Prec@1 39.062 (31.439)	Prec@5 90.625 (84.080)	Acc 0.314 (0.249)	
TRAINING - Epoch: [0][480/781]	Time 0.024 (0.028)	Data 0.000 (0.003)	Loss 1.4459 (1.8062)	Prec@1 45.312 (31.632)	Prec@5 90.625 (84.191)	Acc 0.316 (0.251)	
TRAINING - Epoch: [0][490/781]	Time 0.024 (0.028)	Data 0.000 (0.003)	Loss 1.5809 (1.8010)	Prec@1 39.844 (31.824)	Prec@5 89.844 (84.345)	Acc 0.318 (0.252)	
TRAINING - Epoch: [0][500/781]	Time 0.023 (0.028)	Data 0.000 (0.003)	Loss 1.4364 (1.7962)	Prec@1 42.969 (31.984)	Prec@5 93.750 (84.494)	Acc 0.320 (0.253)	
TRAINING - Epoch: [0][510/781]	Time 0.023 (0.028)	Data 0.000 (0.003)	Loss 1.6264 (1.7932)	Prec@1 30.469 (32.112)	Prec@5 89.844 (84.594)	Acc 0.321 (0.255)	
TRAINING - Epoch: [0][520/781]	Time 0.023 (0.028)	Data 0.000 (0.003)	Loss 1.6339 (1.7901)	Prec@1 42.188 (32.234)	Prec@5 91.406 (84.685)	Acc 0.322 (0.256)	
TRAINING - Epoch: [0][530/781]	Time 0.023 (0.028)	Data 0.000 (0.003)	Loss 1.6809 (1.7867)	Prec@1 39.062 (32.373)	Prec@5 90.625 (84.781)	Acc 0.324 (0.257)	
TRAINING - Epoch: [0][540/781]	Time 0.030 (0.028)	Data 0.000 (0.003)	Loss 1.4705 (1.7822)	Prec@1 48.438 (32.577)	Prec@5 90.625 (84.895)	Acc 0.326 (0.258)	
TRAINING - Epoch: [0][550/781]	Time 0.023 (0.028)	Data 0.000 (0.003)	Loss 1.5937 (1.7776)	Prec@1 42.969 (32.757)	Prec@5 86.719 (85.027)	Acc 0.328 (0.260)	
TRAINING - Epoch: [0][560/781]	Time 0.023 (0.028)	Data 0.000 (0.003)	Loss 1.5776 (1.7733)	Prec@1 37.500 (32.929)	Prec@5 94.531 (85.138)	Acc 0.329 (0.261)	
TRAINING - Epoch: [0][570/781]	Time 0.023 (0.028)	Data 0.000 (0.003)	Loss 1.4405 (1.7683)	Prec@1 43.750 (33.126)	Prec@5 93.750 (85.229)	Acc 0.331 (0.262)	
TRAINING - Epoch: [0][580/781]	Time 0.023 (0.028)	Data 0.000 (0.003)	Loss 1.4053 (1.7637)	Prec@1 53.906 (33.305)	Prec@5 92.188 (85.351)	Acc 0.333 (0.263)	
TRAINING - Epoch: [0][590/781]	Time 0.028 (0.028)	Data 0.000 (0.003)	Loss 1.4459 (1.7588)	Prec@1 42.969 (33.484)	Prec@5 92.188 (85.474)	Acc 0.335 (0.264)	
TRAINING - Epoch: [0][600/781]	Time 0.023 (0.028)	Data 0.000 (0.003)	Loss 1.3140 (1.7538)	Prec@1 50.781 (33.717)	Prec@5 96.094 (85.581)	Acc 0.337 (0.266)	
TRAINING - Epoch: [0][610/781]	Time 0.023 (0.027)	Data 0.000 (0.003)	Loss 1.5780 (1.7491)	Prec@1 42.188 (33.907)	Prec@5 92.188 (85.715)	Acc 0.339 (0.267)	
TRAINING - Epoch: [0][620/781]	Time 0.023 (0.027)	Data 0.000 (0.003)	Loss 1.5386 (1.7461)	Prec@1 42.188 (34.027)	Prec@5 89.062 (85.785)	Acc 0.340 (0.268)	
TRAINING - Epoch: [0][630/781]	Time 0.023 (0.027)	Data 0.000 (0.003)	Loss 1.4721 (1.7422)	Prec@1 50.781 (34.199)	Prec@5 88.281 (85.859)	Acc 0.342 (0.269)	
TRAINING - Epoch: [0][640/781]	Time 0.023 (0.027)	Data 0.000 (0.003)	Loss 1.3383 (1.7373)	Prec@1 50.781 (34.405)	Prec@5 93.750 (85.956)	Acc 0.344 (0.270)	
TRAINING - Epoch: [0][650/781]	Time 0.023 (0.027)	Data 0.000 (0.003)	Loss 1.6600 (1.7325)	Prec@1 42.188 (34.634)	Prec@5 86.719 (86.061)	Acc 0.346 (0.271)	
TRAINING - Epoch: [0][660/781]	Time 0.023 (0.027)	Data 0.000 (0.003)	Loss 1.7024 (1.7289)	Prec@1 35.156 (34.763)	Prec@5 88.281 (86.148)	Acc 0.348 (0.273)	
TRAINING - Epoch: [0][670/781]	Time 0.036 (0.027)	Data 0.000 (0.003)	Loss 1.5620 (1.7251)	Prec@1 46.094 (34.934)	Prec@5 91.406 (86.243)	Acc 0.349 (0.274)	
TRAINING - Epoch: [0][680/781]	Time 0.023 (0.027)	Data 0.000 (0.003)	Loss 1.3712 (1.7206)	Prec@1 42.969 (35.125)	Prec@5 92.188 (86.348)	Acc 0.351 (0.275)	
TRAINING - Epoch: [0][690/781]	Time 0.037 (0.027)	Data 0.000 (0.002)	Loss 1.5835 (1.7167)	Prec@1 41.406 (35.264)	Prec@5 90.625 (86.444)	Acc 0.353 (0.276)	
TRAINING - Epoch: [0][700/781]	Time 0.023 (0.027)	Data 0.000 (0.002)	Loss 1.4247 (1.7123)	Prec@1 45.312 (35.439)	Prec@5 92.188 (86.527)	Acc 0.354 (0.277)	
TRAINING - Epoch: [0][710/781]	Time 0.035 (0.027)	Data 0.000 (0.002)	Loss 1.4985 (1.7087)	Prec@1 43.750 (35.598)	Prec@5 89.844 (86.604)	Acc 0.356 (0.278)	
TRAINING - Epoch: [0][720/781]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 1.5039 (1.7055)	Prec@1 45.312 (35.740)	Prec@5 92.969 (86.670)	Acc 0.357 (0.279)	
TRAINING - Epoch: [0][730/781]	Time 0.023 (0.027)	Data 0.000 (0.002)	Loss 1.3986 (1.7020)	Prec@1 46.875 (35.881)	Prec@5 96.875 (86.753)	Acc 0.359 (0.280)	
TRAINING - Epoch: [0][740/781]	Time 0.023 (0.027)	Data 0.000 (0.002)	Loss 1.4215 (1.6974)	Prec@1 49.219 (36.065)	Prec@5 91.406 (86.827)	Acc 0.361 (0.281)	
TRAINING - Epoch: [0][750/781]	Time 0.035 (0.027)	Data 0.000 (0.002)	Loss 1.4730 (1.6934)	Prec@1 46.875 (36.225)	Prec@5 92.969 (86.911)	Acc 0.362 (0.283)	
TRAINING - Epoch: [0][760/781]	Time 0.023 (0.027)	Data 0.000 (0.002)	Loss 1.5742 (1.6889)	Prec@1 39.844 (36.419)	Prec@5 89.844 (86.998)	Acc 0.364 (0.284)	
TRAINING - Epoch: [0][770/781]	Time 0.023 (0.027)	Data 0.000 (0.002)	Loss 1.3678 (1.6858)	Prec@1 40.625 (36.546)	Prec@5 97.656 (87.086)	Acc 0.365 (0.285)	
TRAINING - Epoch: [0][780/781]	Time 0.023 (0.027)	Data 0.000 (0.002)	Loss 1.3437 (1.6824)	Prec@1 50.000 (36.708)	Prec@5 92.969 (87.151)	Acc 0.367 (0.286)	
Traceback (most recent call last):
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 364, in <module>
    main()
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 130, in main
    main_worker(args)
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 307, in main_worker
    train_results = trainer.train(train_data.get_loader(), chunk_batch=args.chunk_batch)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 282, in train
    return self.forward(data_loader, training=True, average_output=average_output, chunk_batch=chunk_batch)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 276, in forward
    return meter_results(meters)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 193, in meter_results
    results = {name: meter.avg for name, meter in meters.items()}
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 193, in <dictcomp>
    results = {name: meter.avg for name, meter in meters.items()}
AttributeError: 'float' object has no attribute 'avg'
saving to /scratch/tor213/DLS/question4/Hoffer/results/2022-03-31_05-07-01_resnet44_m-4
creating model resnet
created model with configuration: {'dataset': 'cifar10', 'depth': 44}
number of parameters: 661338
/scratch/tor213/.env/lib64/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
optimization regime: [{'epoch': 0, 'optimizer': 'SGD', 'lr': 0.1, 'momentum': 0.9, 'regularizer': {'name': 'WeightDecay', 'value': 0.0001, 'log': False, 'filter': {'parameter_name': <function weight_decay_config.<locals>.<lambda> at 0x1540506b2af0>, 'module': <function weight_decay_config.<locals>.<lambda> at 0x1540506b2940>}}}, {'epoch': 81, 'lr': 0.01}, {'epoch': 122, 'lr': 0.001}, {'epoch': 164, 'lr': 0.0001}]
data regime: Current: {'datasets_path': '~/Datasets', 'name': 'cifar10', 'split': 'train', 'augment': True, 'input_size': None, 'batch_size': 64, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'drop_last': True, 'distributed': False, 'duplicates': 4, 'autoaugment': False, 'cutout': {'holes': 1, 'length': 16}}
 Regime:None

Starting Epoch: 1

Files already downloaded and verified
Files already downloaded and verified
TRAINING - Epoch: [0][0/781]	Time 2.021 (2.021)	Data 1.786 (1.786)	Loss 2.3056 (2.3056)	Prec@1 8.594 (8.594)	Prec@5 42.578 (42.578)	Acc 0.086 (0.086)	
TRAINING - Epoch: [0][10/781]	Time 0.039 (0.220)	Data 0.000 (0.163)	Loss 2.3256 (2.3039)	Prec@1 10.938 (13.707)	Prec@5 53.516 (55.291)	Acc 0.137 (0.120)	
TRAINING - Epoch: [0][20/781]	Time 0.039 (0.134)	Data 0.000 (0.085)	Loss 2.2457 (2.2590)	Prec@1 10.547 (15.439)	Prec@5 63.672 (59.859)	Acc 0.154 (0.135)	
TRAINING - Epoch: [0][30/781]	Time 0.040 (0.104)	Data 0.000 (0.058)	Loss 2.1501 (2.2288)	Prec@1 25.000 (16.784)	Prec@5 68.750 (62.777)	Acc 0.168 (0.144)	
TRAINING - Epoch: [0][40/781]	Time 0.039 (0.088)	Data 0.000 (0.044)	Loss 2.1913 (2.1891)	Prec@1 21.484 (17.864)	Prec@5 74.609 (66.016)	Acc 0.179 (0.151)	
TRAINING - Epoch: [0][50/781]	Time 0.039 (0.078)	Data 0.000 (0.035)	Loss 1.9218 (2.1607)	Prec@1 27.344 (18.735)	Prec@5 81.641 (68.122)	Acc 0.187 (0.158)	
TRAINING - Epoch: [0][60/781]	Time 0.039 (0.072)	Data 0.000 (0.029)	Loss 1.9536 (2.1339)	Prec@1 17.969 (19.294)	Prec@5 83.203 (70.005)	Acc 0.193 (0.163)	
TRAINING - Epoch: [0][70/781]	Time 0.039 (0.067)	Data 0.000 (0.025)	Loss 2.0343 (2.1215)	Prec@1 24.609 (19.663)	Prec@5 80.469 (70.995)	Acc 0.197 (0.167)	
TRAINING - Epoch: [0][80/781]	Time 0.039 (0.064)	Data 0.000 (0.022)	Loss 1.9979 (2.1069)	Prec@1 26.172 (20.187)	Prec@5 77.344 (71.957)	Acc 0.202 (0.171)	
TRAINING - Epoch: [0][90/781]	Time 0.039 (0.061)	Data 0.000 (0.020)	Loss 2.0034 (2.0892)	Prec@1 22.266 (20.768)	Prec@5 76.562 (73.034)	Acc 0.208 (0.175)	
TRAINING - Epoch: [0][100/781]	Time 0.039 (0.059)	Data 0.000 (0.018)	Loss 1.8899 (2.0749)	Prec@1 30.859 (21.589)	Prec@5 79.688 (73.805)	Acc 0.216 (0.179)	
TRAINING - Epoch: [0][110/781]	Time 0.039 (0.057)	Data 0.000 (0.016)	Loss 1.8832 (2.0640)	Prec@1 32.422 (22.118)	Prec@5 83.984 (74.388)	Acc 0.221 (0.182)	
TRAINING - Epoch: [0][120/781]	Time 0.039 (0.056)	Data 0.000 (0.015)	Loss 1.8485 (2.0514)	Prec@1 28.125 (22.508)	Prec@5 82.812 (74.968)	Acc 0.225 (0.186)	
TRAINING - Epoch: [0][130/781]	Time 0.039 (0.055)	Data 0.000 (0.014)	Loss 1.9801 (2.0390)	Prec@1 26.953 (22.966)	Prec@5 80.859 (75.555)	Acc 0.230 (0.189)	
TRAINING - Epoch: [0][140/781]	Time 0.039 (0.054)	Data 0.000 (0.013)	Loss 1.8743 (2.0276)	Prec@1 26.172 (23.269)	Prec@5 86.719 (76.222)	Acc 0.233 (0.192)	
TRAINING - Epoch: [0][150/781]	Time 0.051 (0.053)	Data 0.000 (0.012)	Loss 1.8345 (2.0215)	Prec@1 29.688 (23.647)	Prec@5 85.938 (76.547)	Acc 0.236 (0.195)	
TRAINING - Epoch: [0][160/781]	Time 0.039 (0.052)	Data 0.000 (0.011)	Loss 1.9951 (2.0120)	Prec@1 26.953 (23.959)	Prec@5 80.078 (76.951)	Acc 0.240 (0.197)	
TRAINING - Epoch: [0][170/781]	Time 0.039 (0.051)	Data 0.000 (0.011)	Loss 1.8601 (2.0048)	Prec@1 25.781 (24.171)	Prec@5 78.906 (77.344)	Acc 0.242 (0.200)	
TRAINING - Epoch: [0][180/781]	Time 0.039 (0.051)	Data 0.000 (0.010)	Loss 1.8147 (1.9941)	Prec@1 30.469 (24.605)	Prec@5 83.594 (77.765)	Acc 0.246 (0.202)	
TRAINING - Epoch: [0][190/781]	Time 0.039 (0.050)	Data 0.000 (0.010)	Loss 1.7287 (1.9841)	Prec@1 36.719 (24.894)	Prec@5 87.500 (78.188)	Acc 0.249 (0.205)	
TRAINING - Epoch: [0][200/781]	Time 0.039 (0.049)	Data 0.000 (0.009)	Loss 1.8272 (1.9756)	Prec@1 31.641 (25.068)	Prec@5 85.156 (78.554)	Acc 0.251 (0.207)	
TRAINING - Epoch: [0][210/781]	Time 0.039 (0.049)	Data 0.000 (0.009)	Loss 1.7106 (1.9665)	Prec@1 34.766 (25.374)	Prec@5 90.234 (78.938)	Acc 0.254 (0.209)	
TRAINING - Epoch: [0][220/781]	Time 0.039 (0.049)	Data 0.000 (0.008)	Loss 1.9313 (1.9597)	Prec@1 33.594 (25.709)	Prec@5 81.641 (79.235)	Acc 0.257 (0.211)	
TRAINING - Epoch: [0][230/781]	Time 0.039 (0.048)	Data 0.000 (0.008)	Loss 1.8212 (1.9522)	Prec@1 33.594 (25.960)	Prec@5 85.938 (79.557)	Acc 0.260 (0.213)	
TRAINING - Epoch: [0][240/781]	Time 0.039 (0.048)	Data 0.000 (0.008)	Loss 1.6242 (1.9427)	Prec@1 41.797 (26.311)	Prec@5 89.844 (79.924)	Acc 0.263 (0.215)	
TRAINING - Epoch: [0][250/781]	Time 0.039 (0.048)	Data 0.000 (0.007)	Loss 1.6064 (1.9335)	Prec@1 41.016 (26.640)	Prec@5 89.453 (80.262)	Acc 0.266 (0.217)	
TRAINING - Epoch: [0][260/781]	Time 0.039 (0.047)	Data 0.000 (0.007)	Loss 1.9156 (1.9285)	Prec@1 28.906 (26.850)	Prec@5 80.859 (80.445)	Acc 0.268 (0.219)	
TRAINING - Epoch: [0][270/781]	Time 0.039 (0.047)	Data 0.000 (0.007)	Loss 1.7807 (1.9216)	Prec@1 37.109 (27.109)	Prec@5 84.375 (80.699)	Acc 0.271 (0.221)	
TRAINING - Epoch: [0][280/781]	Time 0.039 (0.047)	Data 0.000 (0.007)	Loss 1.5866 (1.9125)	Prec@1 41.406 (27.487)	Prec@5 93.359 (80.975)	Acc 0.275 (0.223)	
TRAINING - Epoch: [0][290/781]	Time 0.039 (0.046)	Data 0.000 (0.006)	Loss 1.7967 (1.9050)	Prec@1 29.688 (27.792)	Prec@5 88.281 (81.242)	Acc 0.278 (0.225)	
TRAINING - Epoch: [0][300/781]	Time 0.039 (0.046)	Data 0.000 (0.006)	Loss 1.5246 (1.8977)	Prec@1 44.531 (28.069)	Prec@5 92.578 (81.456)	Acc 0.281 (0.227)	
TRAINING - Epoch: [0][310/781]	Time 0.039 (0.046)	Data 0.000 (0.006)	Loss 1.6695 (1.8919)	Prec@1 33.203 (28.335)	Prec@5 91.016 (81.604)	Acc 0.283 (0.228)	
TRAINING - Epoch: [0][320/781]	Time 0.039 (0.046)	Data 0.000 (0.006)	Loss 1.7489 (1.8868)	Prec@1 30.859 (28.517)	Prec@5 83.984 (81.793)	Acc 0.285 (0.230)	
TRAINING - Epoch: [0][330/781]	Time 0.040 (0.046)	Data 0.000 (0.006)	Loss 1.5898 (1.8799)	Prec@1 53.516 (28.795)	Prec@5 86.719 (82.014)	Acc 0.288 (0.232)	
TRAINING - Epoch: [0][340/781]	Time 0.039 (0.045)	Data 0.000 (0.006)	Loss 1.8807 (1.8719)	Prec@1 24.609 (29.124)	Prec@5 84.766 (82.244)	Acc 0.291 (0.234)	
TRAINING - Epoch: [0][350/781]	Time 0.039 (0.045)	Data 0.000 (0.005)	Loss 1.8332 (1.8657)	Prec@1 30.469 (29.378)	Prec@5 85.156 (82.401)	Acc 0.294 (0.235)	
TRAINING - Epoch: [0][360/781]	Time 0.039 (0.045)	Data 0.000 (0.005)	Loss 1.5161 (1.8593)	Prec@1 45.312 (29.612)	Prec@5 92.578 (82.606)	Acc 0.296 (0.237)	
TRAINING - Epoch: [0][370/781]	Time 0.039 (0.045)	Data 0.000 (0.005)	Loss 1.6031 (1.8537)	Prec@1 37.500 (29.843)	Prec@5 88.281 (82.762)	Acc 0.298 (0.239)	
TRAINING - Epoch: [0][380/781]	Time 0.039 (0.045)	Data 0.000 (0.005)	Loss 1.4549 (1.8479)	Prec@1 50.000 (30.102)	Prec@5 93.750 (82.907)	Acc 0.301 (0.240)	
TRAINING - Epoch: [0][390/781]	Time 0.039 (0.045)	Data 0.000 (0.005)	Loss 1.6788 (1.8420)	Prec@1 39.062 (30.376)	Prec@5 91.406 (83.066)	Acc 0.304 (0.242)	
TRAINING - Epoch: [0][400/781]	Time 0.039 (0.045)	Data 0.000 (0.005)	Loss 1.6527 (1.8364)	Prec@1 35.938 (30.615)	Prec@5 91.406 (83.217)	Acc 0.306 (0.243)	
TRAINING - Epoch: [0][410/781]	Time 0.039 (0.044)	Data 0.000 (0.005)	Loss 1.5910 (1.8309)	Prec@1 37.109 (30.843)	Prec@5 92.188 (83.371)	Acc 0.308 (0.245)	
TRAINING - Epoch: [0][420/781]	Time 0.039 (0.044)	Data 0.000 (0.004)	Loss 1.5714 (1.8262)	Prec@1 35.156 (31.053)	Prec@5 92.578 (83.505)	Acc 0.311 (0.246)	
TRAINING - Epoch: [0][430/781]	Time 0.039 (0.044)	Data 0.000 (0.004)	Loss 1.5081 (1.8198)	Prec@1 43.359 (31.296)	Prec@5 94.141 (83.669)	Acc 0.313 (0.248)	
TRAINING - Epoch: [0][440/781]	Time 0.039 (0.044)	Data 0.000 (0.004)	Loss 1.6577 (1.8128)	Prec@1 39.844 (31.597)	Prec@5 89.844 (83.850)	Acc 0.316 (0.249)	
TRAINING - Epoch: [0][450/781]	Time 0.041 (0.044)	Data 0.000 (0.004)	Loss 1.5922 (1.8082)	Prec@1 39.844 (31.785)	Prec@5 89.062 (83.986)	Acc 0.318 (0.251)	
TRAINING - Epoch: [0][460/781]	Time 0.039 (0.044)	Data 0.000 (0.004)	Loss 1.6290 (1.8037)	Prec@1 38.672 (31.977)	Prec@5 90.234 (84.102)	Acc 0.320 (0.252)	
TRAINING - Epoch: [0][470/781]	Time 0.039 (0.044)	Data 0.000 (0.004)	Loss 1.7354 (1.7974)	Prec@1 37.891 (32.230)	Prec@5 91.406 (84.253)	Acc 0.322 (0.254)	
TRAINING - Epoch: [0][480/781]	Time 0.039 (0.044)	Data 0.000 (0.004)	Loss 1.4345 (1.7929)	Prec@1 47.656 (32.445)	Prec@5 91.016 (84.379)	Acc 0.324 (0.255)	
TRAINING - Epoch: [0][490/781]	Time 0.039 (0.044)	Data 0.000 (0.004)	Loss 1.4920 (1.7872)	Prec@1 45.312 (32.656)	Prec@5 91.406 (84.532)	Acc 0.327 (0.257)	
TRAINING - Epoch: [0][500/781]	Time 0.039 (0.044)	Data 0.000 (0.004)	Loss 1.3811 (1.7821)	Prec@1 47.656 (32.843)	Prec@5 93.750 (84.692)	Acc 0.328 (0.258)	
TRAINING - Epoch: [0][510/781]	Time 0.047 (0.043)	Data 0.000 (0.004)	Loss 1.4977 (1.7781)	Prec@1 42.188 (33.006)	Prec@5 90.234 (84.794)	Acc 0.330 (0.260)	
TRAINING - Epoch: [0][520/781]	Time 0.039 (0.043)	Data 0.000 (0.004)	Loss 1.4841 (1.7747)	Prec@1 47.656 (33.126)	Prec@5 92.969 (84.885)	Acc 0.331 (0.261)	
TRAINING - Epoch: [0][530/781]	Time 0.039 (0.043)	Data 0.000 (0.004)	Loss 1.6074 (1.7708)	Prec@1 34.375 (33.271)	Prec@5 91.016 (84.993)	Acc 0.333 (0.262)	
TRAINING - Epoch: [0][540/781]	Time 0.039 (0.043)	Data 0.000 (0.004)	Loss 1.4306 (1.7652)	Prec@1 47.656 (33.495)	Prec@5 93.359 (85.135)	Acc 0.335 (0.264)	
TRAINING - Epoch: [0][550/781]	Time 0.039 (0.043)	Data 0.000 (0.003)	Loss 1.4171 (1.7603)	Prec@1 49.219 (33.709)	Prec@5 90.625 (85.268)	Acc 0.337 (0.265)	
TRAINING - Epoch: [0][560/781]	Time 0.039 (0.043)	Data 0.000 (0.003)	Loss 1.5688 (1.7555)	Prec@1 45.312 (33.914)	Prec@5 92.969 (85.385)	Acc 0.339 (0.266)	
TRAINING - Epoch: [0][570/781]	Time 0.039 (0.043)	Data 0.000 (0.003)	Loss 1.4806 (1.7503)	Prec@1 42.188 (34.150)	Prec@5 96.875 (85.512)	Acc 0.341 (0.267)	
TRAINING - Epoch: [0][580/781]	Time 0.039 (0.043)	Data 0.000 (0.003)	Loss 1.4275 (1.7453)	Prec@1 51.562 (34.317)	Prec@5 90.625 (85.641)	Acc 0.343 (0.269)	
TRAINING - Epoch: [0][590/781]	Time 0.039 (0.043)	Data 0.000 (0.003)	Loss 1.5461 (1.7404)	Prec@1 39.844 (34.512)	Prec@5 91.016 (85.756)	Acc 0.345 (0.270)	
TRAINING - Epoch: [0][600/781]	Time 0.041 (0.043)	Data 0.000 (0.003)	Loss 1.3473 (1.7352)	Prec@1 44.922 (34.720)	Prec@5 96.094 (85.871)	Acc 0.347 (0.271)	
TRAINING - Epoch: [0][610/781]	Time 0.039 (0.043)	Data 0.000 (0.003)	Loss 1.5138 (1.7305)	Prec@1 43.750 (34.897)	Prec@5 90.234 (85.989)	Acc 0.349 (0.273)	
TRAINING - Epoch: [0][620/781]	Time 0.039 (0.043)	Data 0.000 (0.003)	Loss 1.5308 (1.7266)	Prec@1 45.312 (35.054)	Prec@5 87.500 (86.080)	Acc 0.351 (0.274)	
TRAINING - Epoch: [0][630/781]	Time 0.039 (0.043)	Data 0.000 (0.003)	Loss 1.5357 (1.7229)	Prec@1 45.312 (35.211)	Prec@5 90.234 (86.168)	Acc 0.352 (0.275)	
TRAINING - Epoch: [0][640/781]	Time 0.039 (0.043)	Data 0.000 (0.003)	Loss 1.2424 (1.7178)	Prec@1 53.516 (35.425)	Prec@5 96.484 (86.279)	Acc 0.354 (0.276)	
TRAINING - Epoch: [0][650/781]	Time 0.039 (0.043)	Data 0.000 (0.003)	Loss 1.5690 (1.7125)	Prec@1 47.656 (35.643)	Prec@5 88.281 (86.388)	Acc 0.356 (0.277)	
TRAINING - Epoch: [0][660/781]	Time 0.040 (0.043)	Data 0.000 (0.003)	Loss 1.5988 (1.7081)	Prec@1 42.188 (35.818)	Prec@5 91.016 (86.490)	Acc 0.358 (0.279)	
TRAINING - Epoch: [0][670/781]	Time 0.039 (0.043)	Data 0.000 (0.003)	Loss 1.4662 (1.7040)	Prec@1 47.266 (35.995)	Prec@5 93.359 (86.581)	Acc 0.360 (0.280)	
TRAINING - Epoch: [0][680/781]	Time 0.039 (0.042)	Data 0.000 (0.003)	Loss 1.3436 (1.6991)	Prec@1 48.828 (36.181)	Prec@5 92.188 (86.680)	Acc 0.362 (0.281)	
TRAINING - Epoch: [0][690/781]	Time 0.039 (0.042)	Data 0.000 (0.003)	Loss 1.4711 (1.6947)	Prec@1 44.531 (36.335)	Prec@5 90.234 (86.766)	Acc 0.363 (0.282)	
TRAINING - Epoch: [0][700/781]	Time 0.039 (0.042)	Data 0.000 (0.003)	Loss 1.3408 (1.6906)	Prec@1 44.531 (36.490)	Prec@5 91.406 (86.846)	Acc 0.365 (0.283)	
TRAINING - Epoch: [0][710/781]	Time 0.039 (0.042)	Data 0.000 (0.003)	Loss 1.3622 (1.6867)	Prec@1 49.609 (36.644)	Prec@5 93.359 (86.925)	Acc 0.366 (0.285)	
TRAINING - Epoch: [0][720/781]	Time 0.039 (0.042)	Data 0.000 (0.003)	Loss 1.4044 (1.6834)	Prec@1 48.438 (36.773)	Prec@5 93.359 (87.002)	Acc 0.368 (0.286)	
TRAINING - Epoch: [0][730/781]	Time 0.039 (0.042)	Data 0.000 (0.003)	Loss 1.4136 (1.6794)	Prec@1 43.750 (36.919)	Prec@5 96.875 (87.092)	Acc 0.369 (0.287)	
TRAINING - Epoch: [0][740/781]	Time 0.045 (0.042)	Data 0.000 (0.003)	Loss 1.4747 (1.6754)	Prec@1 46.094 (37.071)	Prec@5 93.750 (87.175)	Acc 0.371 (0.288)	
TRAINING - Epoch: [0][750/781]	Time 0.039 (0.042)	Data 0.000 (0.003)	Loss 1.5845 (1.6715)	Prec@1 42.578 (37.217)	Prec@5 91.016 (87.258)	Acc 0.372 (0.289)	
TRAINING - Epoch: [0][760/781]	Time 0.039 (0.042)	Data 0.000 (0.003)	Loss 1.4750 (1.6673)	Prec@1 49.219 (37.410)	Prec@5 89.844 (87.332)	Acc 0.374 (0.290)	
TRAINING - Epoch: [0][770/781]	Time 0.039 (0.042)	Data 0.000 (0.003)	Loss 1.2684 (1.6635)	Prec@1 50.000 (37.537)	Prec@5 97.266 (87.429)	Acc 0.375 (0.291)	
TRAINING - Epoch: [0][780/781]	Time 0.039 (0.042)	Data 0.000 (0.003)	Loss 1.3518 (1.6604)	Prec@1 53.125 (37.697)	Prec@5 94.141 (87.497)	Acc 0.377 (0.292)	
Traceback (most recent call last):
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 364, in <module>
    main()
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 130, in main
    main_worker(args)
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 307, in main_worker
    train_results = trainer.train(train_data.get_loader(), chunk_batch=args.chunk_batch)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 282, in train
    return self.forward(data_loader, training=True, average_output=average_output, chunk_batch=chunk_batch)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 276, in forward
    return meter_results(meters)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 193, in meter_results
    results = {name: meter.avg for name, meter in meters.items()}
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 193, in <dictcomp>
    results = {name: meter.avg for name, meter in meters.items()}
AttributeError: 'float' object has no attribute 'avg'
saving to /scratch/tor213/DLS/question4/Hoffer/results/2022-03-31_05-07-41_resnet44_m-8
creating model resnet
created model with configuration: {'dataset': 'cifar10', 'depth': 44}
number of parameters: 661338
/scratch/tor213/.env/lib64/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
optimization regime: [{'epoch': 0, 'optimizer': 'SGD', 'lr': 0.1, 'momentum': 0.9, 'regularizer': {'name': 'WeightDecay', 'value': 0.0001, 'log': False, 'filter': {'parameter_name': <function weight_decay_config.<locals>.<lambda> at 0x1505d65dcaf0>, 'module': <function weight_decay_config.<locals>.<lambda> at 0x1505d65dc940>}}}, {'epoch': 81, 'lr': 0.01}, {'epoch': 122, 'lr': 0.001}, {'epoch': 164, 'lr': 0.0001}]
data regime: Current: {'datasets_path': '~/Datasets', 'name': 'cifar10', 'split': 'train', 'augment': True, 'input_size': None, 'batch_size': 64, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'drop_last': True, 'distributed': False, 'duplicates': 8, 'autoaugment': False, 'cutout': {'holes': 1, 'length': 16}}
 Regime:None

Starting Epoch: 1

Files already downloaded and verified
Files already downloaded and verified
TRAINING - Epoch: [0][0/781]	Time 2.343 (2.343)	Data 1.854 (1.854)	Loss 2.3068 (2.3068)	Prec@1 7.031 (7.031)	Prec@5 42.383 (42.383)	Acc 0.070 (0.070)	
TRAINING - Epoch: [0][10/781]	Time 0.072 (0.281)	Data 0.000 (0.169)	Loss 2.3304 (2.3035)	Prec@1 11.328 (13.903)	Prec@5 53.516 (55.273)	Acc 0.139 (0.121)	
TRAINING - Epoch: [0][20/781]	Time 0.072 (0.182)	Data 0.000 (0.088)	Loss 2.2554 (2.2595)	Prec@1 9.961 (15.597)	Prec@5 63.867 (59.552)	Acc 0.156 (0.137)	
TRAINING - Epoch: [0][30/781]	Time 0.072 (0.147)	Data 0.000 (0.060)	Loss 2.1634 (2.2303)	Prec@1 25.781 (17.030)	Prec@5 67.578 (62.393)	Acc 0.170 (0.145)	
TRAINING - Epoch: [0][40/781]	Time 0.072 (0.129)	Data 0.000 (0.045)	Loss 2.1924 (2.1897)	Prec@1 21.875 (17.964)	Prec@5 74.023 (65.897)	Acc 0.180 (0.153)	
TRAINING - Epoch: [0][50/781]	Time 0.073 (0.118)	Data 0.000 (0.037)	Loss 1.9228 (2.1597)	Prec@1 24.023 (18.869)	Prec@5 79.688 (67.934)	Acc 0.189 (0.159)	
TRAINING - Epoch: [0][60/781]	Time 0.072 (0.110)	Data 0.000 (0.031)	Loss 1.9263 (2.1325)	Prec@1 21.484 (19.685)	Prec@5 82.422 (69.771)	Acc 0.197 (0.165)	
TRAINING - Epoch: [0][70/781]	Time 0.074 (0.105)	Data 0.000 (0.026)	Loss 2.0323 (2.1187)	Prec@1 24.414 (19.936)	Prec@5 82.422 (71.028)	Acc 0.199 (0.170)	
TRAINING - Epoch: [0][80/781]	Time 0.072 (0.101)	Data 0.000 (0.023)	Loss 1.9880 (2.1035)	Prec@1 27.344 (20.566)	Prec@5 72.656 (72.046)	Acc 0.206 (0.174)	
TRAINING - Epoch: [0][90/781]	Time 0.072 (0.098)	Data 0.000 (0.021)	Loss 2.0190 (2.0872)	Prec@1 21.875 (21.135)	Prec@5 73.438 (72.993)	Acc 0.211 (0.177)	
TRAINING - Epoch: [0][100/781]	Time 0.073 (0.096)	Data 0.000 (0.019)	Loss 1.9204 (2.0709)	Prec@1 28.125 (21.889)	Prec@5 78.320 (73.731)	Acc 0.219 (0.181)	
TRAINING - Epoch: [0][110/781]	Time 0.074 (0.094)	Data 0.000 (0.017)	Loss 1.8843 (2.0585)	Prec@1 28.516 (22.413)	Prec@5 79.883 (74.275)	Acc 0.224 (0.185)	
TRAINING - Epoch: [0][120/781]	Time 0.073 (0.092)	Data 0.000 (0.016)	Loss 1.8461 (2.0447)	Prec@1 26.562 (22.781)	Prec@5 84.180 (74.924)	Acc 0.228 (0.188)	
TRAINING - Epoch: [0][130/781]	Time 0.072 (0.091)	Data 0.000 (0.014)	Loss 1.9486 (2.0319)	Prec@1 30.469 (23.221)	Prec@5 83.594 (75.598)	Acc 0.232 (0.192)	
TRAINING - Epoch: [0][140/781]	Time 0.072 (0.089)	Data 0.000 (0.013)	Loss 1.8409 (2.0199)	Prec@1 30.273 (23.629)	Prec@5 85.156 (76.226)	Acc 0.236 (0.195)	
TRAINING - Epoch: [0][150/781]	Time 0.073 (0.088)	Data 0.000 (0.012)	Loss 1.8083 (2.0127)	Prec@1 30.078 (23.855)	Prec@5 85.156 (76.670)	Acc 0.239 (0.197)	
TRAINING - Epoch: [0][160/781]	Time 0.073 (0.088)	Data 0.000 (0.012)	Loss 1.9941 (2.0031)	Prec@1 27.344 (24.169)	Prec@5 78.320 (77.099)	Acc 0.242 (0.200)	
TRAINING - Epoch: [0][170/781]	Time 0.073 (0.087)	Data 0.000 (0.011)	Loss 1.8497 (1.9947)	Prec@1 23.633 (24.414)	Prec@5 81.055 (77.522)	Acc 0.244 (0.203)	
TRAINING - Epoch: [0][180/781]	Time 0.072 (0.086)	Data 0.000 (0.010)	Loss 1.7864 (1.9839)	Prec@1 26.953 (24.803)	Prec@5 86.719 (77.938)	Acc 0.248 (0.205)	
TRAINING - Epoch: [0][190/781]	Time 0.073 (0.085)	Data 0.000 (0.010)	Loss 1.8551 (1.9751)	Prec@1 35.938 (25.058)	Prec@5 81.250 (78.365)	Acc 0.251 (0.207)	
TRAINING - Epoch: [0][200/781]	Time 0.072 (0.085)	Data 0.000 (0.009)	Loss 1.8580 (1.9670)	Prec@1 30.859 (25.215)	Prec@5 84.570 (78.759)	Acc 0.252 (0.210)	
TRAINING - Epoch: [0][210/781]	Time 0.072 (0.084)	Data 0.000 (0.009)	Loss 1.7019 (1.9588)	Prec@1 32.227 (25.411)	Prec@5 91.211 (79.121)	Acc 0.254 (0.212)	
TRAINING - Epoch: [0][220/781]	Time 0.072 (0.084)	Data 0.000 (0.009)	Loss 1.9295 (1.9513)	Prec@1 32.031 (25.719)	Prec@5 82.812 (79.410)	Acc 0.257 (0.214)	
TRAINING - Epoch: [0][230/781]	Time 0.074 (0.083)	Data 0.000 (0.008)	Loss 1.7931 (1.9443)	Prec@1 32.422 (25.963)	Prec@5 87.305 (79.733)	Acc 0.260 (0.216)	
TRAINING - Epoch: [0][240/781]	Time 0.072 (0.083)	Data 0.000 (0.008)	Loss 1.5905 (1.9350)	Prec@1 42.578 (26.355)	Prec@5 90.039 (80.053)	Acc 0.264 (0.217)	
TRAINING - Epoch: [0][250/781]	Time 0.072 (0.083)	Data 0.000 (0.008)	Loss 1.6903 (1.9263)	Prec@1 36.914 (26.686)	Prec@5 86.719 (80.385)	Acc 0.267 (0.219)	
TRAINING - Epoch: [0][260/781]	Time 0.072 (0.082)	Data 0.000 (0.007)	Loss 1.9651 (1.9216)	Prec@1 29.688 (26.854)	Prec@5 77.930 (80.552)	Acc 0.269 (0.221)	
TRAINING - Epoch: [0][270/781]	Time 0.072 (0.082)	Data 0.000 (0.007)	Loss 1.7796 (1.9153)	Prec@1 33.398 (27.088)	Prec@5 84.766 (80.805)	Acc 0.271 (0.223)	
TRAINING - Epoch: [0][280/781]	Time 0.072 (0.082)	Data 0.000 (0.007)	Loss 1.5850 (1.9066)	Prec@1 47.461 (27.435)	Prec@5 89.453 (81.061)	Acc 0.274 (0.225)	
TRAINING - Epoch: [0][290/781]	Time 0.072 (0.081)	Data 0.000 (0.007)	Loss 1.8977 (1.8997)	Prec@1 27.344 (27.720)	Prec@5 84.961 (81.328)	Acc 0.277 (0.227)	
TRAINING - Epoch: [0][300/781]	Time 0.072 (0.081)	Data 0.000 (0.006)	Loss 1.5497 (1.8921)	Prec@1 41.211 (27.986)	Prec@5 89.648 (81.561)	Acc 0.280 (0.228)	
TRAINING - Epoch: [0][310/781]	Time 0.073 (0.081)	Data 0.000 (0.006)	Loss 1.6933 (1.8863)	Prec@1 34.375 (28.290)	Prec@5 88.867 (81.726)	Acc 0.283 (0.230)	
TRAINING - Epoch: [0][320/781]	Time 0.072 (0.081)	Data 0.000 (0.006)	Loss 1.8401 (1.8815)	Prec@1 30.859 (28.480)	Prec@5 81.641 (81.881)	Acc 0.285 (0.232)	
TRAINING - Epoch: [0][330/781]	Time 0.073 (0.080)	Data 0.000 (0.006)	Loss 1.6443 (1.8755)	Prec@1 44.922 (28.713)	Prec@5 88.086 (82.087)	Acc 0.287 (0.233)	
TRAINING - Epoch: [0][340/781]	Time 0.072 (0.080)	Data 0.000 (0.006)	Loss 1.7875 (1.8679)	Prec@1 35.547 (29.044)	Prec@5 88.477 (82.315)	Acc 0.290 (0.235)	
TRAINING - Epoch: [0][350/781]	Time 0.075 (0.080)	Data 0.000 (0.005)	Loss 1.8123 (1.8620)	Prec@1 30.859 (29.306)	Prec@5 83.594 (82.476)	Acc 0.293 (0.237)	
TRAINING - Epoch: [0][360/781]	Time 0.073 (0.080)	Data 0.000 (0.005)	Loss 1.4970 (1.8551)	Prec@1 38.281 (29.566)	Prec@5 93.555 (82.710)	Acc 0.296 (0.238)	
TRAINING - Epoch: [0][370/781]	Time 0.072 (0.080)	Data 0.000 (0.005)	Loss 1.6165 (1.8492)	Prec@1 44.531 (29.802)	Prec@5 89.453 (82.884)	Acc 0.298 (0.240)	
TRAINING - Epoch: [0][380/781]	Time 0.072 (0.079)	Data 0.000 (0.005)	Loss 1.4557 (1.8423)	Prec@1 46.484 (30.092)	Prec@5 95.312 (83.071)	Acc 0.301 (0.241)	
TRAINING - Epoch: [0][390/781]	Time 0.073 (0.079)	Data 0.000 (0.005)	Loss 1.6685 (1.8359)	Prec@1 36.719 (30.343)	Prec@5 91.406 (83.248)	Acc 0.303 (0.243)	
TRAINING - Epoch: [0][400/781]	Time 0.073 (0.079)	Data 0.000 (0.005)	Loss 1.6591 (1.8308)	Prec@1 39.258 (30.607)	Prec@5 93.164 (83.414)	Acc 0.306 (0.244)	
TRAINING - Epoch: [0][410/781]	Time 0.072 (0.079)	Data 0.000 (0.005)	Loss 1.6242 (1.8259)	Prec@1 35.742 (30.808)	Prec@5 91.992 (83.541)	Acc 0.308 (0.246)	
TRAINING - Epoch: [0][420/781]	Time 0.072 (0.079)	Data 0.000 (0.005)	Loss 1.5821 (1.8207)	Prec@1 35.547 (31.053)	Prec@5 93.750 (83.683)	Acc 0.311 (0.247)	
TRAINING - Epoch: [0][430/781]	Time 0.073 (0.079)	Data 0.000 (0.004)	Loss 1.5080 (1.8144)	Prec@1 41.602 (31.327)	Prec@5 95.312 (83.845)	Acc 0.313 (0.249)	
TRAINING - Epoch: [0][440/781]	Time 0.072 (0.079)	Data 0.000 (0.004)	Loss 1.7229 (1.8074)	Prec@1 39.258 (31.637)	Prec@5 89.844 (84.018)	Acc 0.316 (0.250)	
TRAINING - Epoch: [0][450/781]	Time 0.072 (0.079)	Data 0.000 (0.004)	Loss 1.5379 (1.8022)	Prec@1 42.188 (31.848)	Prec@5 90.234 (84.164)	Acc 0.318 (0.252)	
TRAINING - Epoch: [0][460/781]	Time 0.078 (0.078)	Data 0.000 (0.004)	Loss 1.6869 (1.7976)	Prec@1 41.016 (32.092)	Prec@5 88.672 (84.270)	Acc 0.321 (0.253)	
TRAINING - Epoch: [0][470/781]	Time 0.072 (0.078)	Data 0.000 (0.004)	Loss 1.6560 (1.7911)	Prec@1 40.430 (32.360)	Prec@5 88.867 (84.422)	Acc 0.324 (0.255)	
TRAINING - Epoch: [0][480/781]	Time 0.072 (0.078)	Data 0.000 (0.004)	Loss 1.4785 (1.7865)	Prec@1 41.797 (32.549)	Prec@5 90.625 (84.550)	Acc 0.325 (0.256)	
TRAINING - Epoch: [0][490/781]	Time 0.073 (0.078)	Data 0.000 (0.004)	Loss 1.5671 (1.7809)	Prec@1 44.336 (32.768)	Prec@5 88.086 (84.694)	Acc 0.328 (0.258)	
TRAINING - Epoch: [0][500/781]	Time 0.073 (0.078)	Data 0.000 (0.004)	Loss 1.3464 (1.7752)	Prec@1 48.438 (32.985)	Prec@5 93.555 (84.847)	Acc 0.330 (0.259)	
TRAINING - Epoch: [0][510/781]	Time 0.072 (0.078)	Data 0.000 (0.004)	Loss 1.5034 (1.7704)	Prec@1 43.945 (33.168)	Prec@5 91.992 (84.978)	Acc 0.332 (0.261)	
TRAINING - Epoch: [0][520/781]	Time 0.074 (0.078)	Data 0.000 (0.004)	Loss 1.5681 (1.7663)	Prec@1 41.406 (33.338)	Prec@5 89.648 (85.086)	Acc 0.333 (0.262)	
TRAINING - Epoch: [0][530/781]	Time 0.073 (0.078)	Data 0.000 (0.004)	Loss 1.5666 (1.7614)	Prec@1 35.547 (33.522)	Prec@5 91.992 (85.220)	Acc 0.335 (0.263)	
TRAINING - Epoch: [0][540/781]	Time 0.072 (0.078)	Data 0.000 (0.004)	Loss 1.3633 (1.7553)	Prec@1 53.125 (33.781)	Prec@5 92.578 (85.356)	Acc 0.338 (0.265)	
TRAINING - Epoch: [0][550/781]	Time 0.073 (0.078)	Data 0.000 (0.004)	Loss 1.5283 (1.7507)	Prec@1 42.969 (33.973)	Prec@5 91.406 (85.471)	Acc 0.340 (0.266)	
TRAINING - Epoch: [0][560/781]	Time 0.073 (0.077)	Data 0.000 (0.003)	Loss 1.5247 (1.7455)	Prec@1 40.625 (34.184)	Prec@5 91.797 (85.588)	Acc 0.342 (0.267)	
TRAINING - Epoch: [0][570/781]	Time 0.072 (0.077)	Data 0.000 (0.003)	Loss 1.4128 (1.7398)	Prec@1 45.312 (34.443)	Prec@5 94.531 (85.716)	Acc 0.344 (0.269)	
TRAINING - Epoch: [0][580/781]	Time 0.073 (0.077)	Data 0.000 (0.003)	Loss 1.3220 (1.7339)	Prec@1 55.469 (34.642)	Prec@5 92.969 (85.861)	Acc 0.346 (0.270)	
TRAINING - Epoch: [0][590/781]	Time 0.072 (0.077)	Data 0.000 (0.003)	Loss 1.4397 (1.7282)	Prec@1 43.945 (34.855)	Prec@5 91.211 (86.003)	Acc 0.349 (0.271)	
TRAINING - Epoch: [0][600/781]	Time 0.073 (0.077)	Data 0.000 (0.003)	Loss 1.3276 (1.7228)	Prec@1 48.633 (35.066)	Prec@5 95.508 (86.123)	Acc 0.351 (0.273)	
TRAINING - Epoch: [0][610/781]	Time 0.073 (0.077)	Data 0.000 (0.003)	Loss 1.5579 (1.7177)	Prec@1 41.211 (35.249)	Prec@5 90.625 (86.247)	Acc 0.352 (0.274)	
TRAINING - Epoch: [0][620/781]	Time 0.072 (0.077)	Data 0.000 (0.003)	Loss 1.4837 (1.7142)	Prec@1 44.727 (35.375)	Prec@5 90.234 (86.321)	Acc 0.354 (0.275)	
TRAINING - Epoch: [0][630/781]	Time 0.073 (0.077)	Data 0.000 (0.003)	Loss 1.3718 (1.7100)	Prec@1 51.953 (35.562)	Prec@5 92.383 (86.409)	Acc 0.356 (0.276)	
TRAINING - Epoch: [0][640/781]	Time 0.072 (0.077)	Data 0.000 (0.003)	Loss 1.2128 (1.7046)	Prec@1 53.711 (35.798)	Prec@5 95.898 (86.514)	Acc 0.358 (0.278)	
TRAINING - Epoch: [0][650/781]	Time 0.072 (0.077)	Data 0.000 (0.003)	Loss 1.5704 (1.6988)	Prec@1 46.289 (36.060)	Prec@5 87.500 (86.622)	Acc 0.361 (0.279)	
TRAINING - Epoch: [0][660/781]	Time 0.073 (0.077)	Data 0.000 (0.003)	Loss 1.5577 (1.6948)	Prec@1 44.531 (36.234)	Prec@5 91.797 (86.716)	Acc 0.362 (0.280)	
TRAINING - Epoch: [0][670/781]	Time 0.073 (0.077)	Data 0.000 (0.003)	Loss 1.4690 (1.6906)	Prec@1 46.875 (36.425)	Prec@5 92.969 (86.798)	Acc 0.364 (0.281)	
TRAINING - Epoch: [0][680/781]	Time 0.074 (0.077)	Data 0.000 (0.003)	Loss 1.3268 (1.6859)	Prec@1 51.758 (36.613)	Prec@5 91.602 (86.897)	Acc 0.366 (0.283)	
TRAINING - Epoch: [0][690/781]	Time 0.072 (0.077)	Data 0.000 (0.003)	Loss 1.3827 (1.6812)	Prec@1 49.219 (36.790)	Prec@5 90.430 (86.991)	Acc 0.368 (0.284)	
TRAINING - Epoch: [0][700/781]	Time 0.072 (0.077)	Data 0.000 (0.003)	Loss 1.3335 (1.6766)	Prec@1 49.805 (36.975)	Prec@5 91.406 (87.087)	Acc 0.370 (0.285)	
TRAINING - Epoch: [0][710/781]	Time 0.073 (0.077)	Data 0.000 (0.003)	Loss 1.3536 (1.6724)	Prec@1 48.047 (37.139)	Prec@5 92.969 (87.180)	Acc 0.371 (0.286)	
TRAINING - Epoch: [0][720/781]	Time 0.073 (0.077)	Data 0.000 (0.003)	Loss 1.2749 (1.6683)	Prec@1 52.734 (37.305)	Prec@5 95.508 (87.265)	Acc 0.373 (0.288)	
TRAINING - Epoch: [0][730/781]	Time 0.074 (0.076)	Data 0.000 (0.003)	Loss 1.3402 (1.6642)	Prec@1 45.117 (37.467)	Prec@5 96.289 (87.347)	Acc 0.375 (0.289)	
TRAINING - Epoch: [0][740/781]	Time 0.072 (0.076)	Data 0.000 (0.003)	Loss 1.4183 (1.6598)	Prec@1 50.391 (37.658)	Prec@5 94.141 (87.438)	Acc 0.377 (0.290)	
TRAINING - Epoch: [0][750/781]	Time 0.073 (0.076)	Data 0.000 (0.003)	Loss 1.4713 (1.6558)	Prec@1 48.047 (37.828)	Prec@5 91.406 (87.511)	Acc 0.378 (0.291)	
TRAINING - Epoch: [0][760/781]	Time 0.072 (0.076)	Data 0.000 (0.003)	Loss 1.3983 (1.6512)	Prec@1 50.391 (38.034)	Prec@5 92.188 (87.595)	Acc 0.380 (0.292)	
TRAINING - Epoch: [0][770/781]	Time 0.072 (0.076)	Data 0.000 (0.003)	Loss 1.2471 (1.6472)	Prec@1 48.633 (38.194)	Prec@5 95.703 (87.693)	Acc 0.382 (0.293)	
TRAINING - Epoch: [0][780/781]	Time 0.072 (0.076)	Data 0.000 (0.003)	Loss 1.3001 (1.6436)	Prec@1 56.445 (38.354)	Prec@5 91.406 (87.755)	Acc 0.384 (0.295)	
Traceback (most recent call last):
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 364, in <module>
    main()
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 130, in main
    main_worker(args)
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 307, in main_worker
    train_results = trainer.train(train_data.get_loader(), chunk_batch=args.chunk_batch)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 282, in train
    return self.forward(data_loader, training=True, average_output=average_output, chunk_batch=chunk_batch)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 276, in forward
    return meter_results(meters)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 193, in meter_results
    results = {name: meter.avg for name, meter in meters.items()}
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 193, in <dictcomp>
    results = {name: meter.avg for name, meter in meters.items()}
AttributeError: 'float' object has no attribute 'avg'
saving to /scratch/tor213/DLS/question4/Hoffer/results/2022-03-31_05-08-47_resnet44_m-16
creating model resnet
created model with configuration: {'dataset': 'cifar10', 'depth': 44}
number of parameters: 661338
/scratch/tor213/.env/lib64/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
optimization regime: [{'epoch': 0, 'optimizer': 'SGD', 'lr': 0.1, 'momentum': 0.9, 'regularizer': {'name': 'WeightDecay', 'value': 0.0001, 'log': False, 'filter': {'parameter_name': <function weight_decay_config.<locals>.<lambda> at 0x149bcfff7af0>, 'module': <function weight_decay_config.<locals>.<lambda> at 0x149bcfff7940>}}}, {'epoch': 81, 'lr': 0.01}, {'epoch': 122, 'lr': 0.001}, {'epoch': 164, 'lr': 0.0001}]
data regime: Current: {'datasets_path': '~/Datasets', 'name': 'cifar10', 'split': 'train', 'augment': True, 'input_size': None, 'batch_size': 64, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'drop_last': True, 'distributed': False, 'duplicates': 16, 'autoaugment': False, 'cutout': {'holes': 1, 'length': 16}}
 Regime:None

Starting Epoch: 1

Files already downloaded and verified
Files already downloaded and verified
TRAINING - Epoch: [0][0/781]	Time 3.132 (3.132)	Data 2.323 (2.323)	Loss 2.3063 (2.3063)	Prec@1 8.203 (8.203)	Prec@5 41.309 (41.309)	Acc 0.082 (0.082)	
TRAINING - Epoch: [0][10/781]	Time 0.137 (0.409)	Data 0.000 (0.211)	Loss 2.3270 (2.3034)	Prec@1 10.938 (13.707)	Prec@5 53.906 (55.513)	Acc 0.137 (0.122)	
TRAINING - Epoch: [0][20/781]	Time 0.137 (0.280)	Data 0.000 (0.111)	Loss 2.2528 (2.2592)	Prec@1 10.742 (15.332)	Prec@5 64.844 (59.761)	Acc 0.153 (0.136)	
TRAINING - Epoch: [0][30/781]	Time 0.137 (0.233)	Data 0.000 (0.075)	Loss 2.1510 (2.2293)	Prec@1 26.074 (16.784)	Prec@5 67.090 (62.582)	Acc 0.168 (0.144)	
TRAINING - Epoch: [0][40/781]	Time 0.137 (0.210)	Data 0.000 (0.057)	Loss 2.1896 (2.1883)	Prec@1 20.215 (17.726)	Prec@5 75.293 (66.118)	Acc 0.177 (0.151)	
TRAINING - Epoch: [0][50/781]	Time 0.139 (0.196)	Data 0.000 (0.046)	Loss 1.9108 (2.1585)	Prec@1 28.320 (18.827)	Prec@5 80.176 (68.252)	Acc 0.188 (0.158)	
TRAINING - Epoch: [0][60/781]	Time 0.137 (0.186)	Data 0.000 (0.038)	Loss 1.9346 (2.1326)	Prec@1 21.582 (19.541)	Prec@5 83.691 (69.978)	Acc 0.195 (0.163)	
TRAINING - Epoch: [0][70/781]	Time 0.137 (0.179)	Data 0.000 (0.033)	Loss 2.0168 (2.1188)	Prec@1 26.172 (19.854)	Prec@5 80.762 (71.164)	Acc 0.199 (0.168)	
TRAINING - Epoch: [0][80/781]	Time 0.137 (0.174)	Data 0.000 (0.029)	Loss 2.0020 (2.1035)	Prec@1 25.684 (20.410)	Prec@5 74.512 (72.203)	Acc 0.204 (0.172)	
TRAINING - Epoch: [0][90/781]	Time 0.137 (0.170)	Data 0.000 (0.026)	Loss 2.0049 (2.0869)	Prec@1 23.340 (20.969)	Prec@5 77.051 (73.260)	Acc 0.210 (0.176)	
TRAINING - Epoch: [0][100/781]	Time 0.137 (0.166)	Data 0.000 (0.023)	Loss 1.9305 (2.0713)	Prec@1 29.395 (21.735)	Prec@5 77.930 (74.051)	Acc 0.217 (0.180)	
TRAINING - Epoch: [0][110/781]	Time 0.137 (0.164)	Data 0.000 (0.021)	Loss 1.8541 (2.0581)	Prec@1 30.273 (22.233)	Prec@5 81.152 (74.591)	Acc 0.222 (0.183)	
TRAINING - Epoch: [0][120/781]	Time 0.137 (0.162)	Data 0.000 (0.019)	Loss 1.8389 (2.0457)	Prec@1 27.246 (22.567)	Prec@5 84.375 (75.228)	Acc 0.226 (0.187)	
TRAINING - Epoch: [0][130/781]	Time 0.137 (0.160)	Data 0.000 (0.018)	Loss 1.8537 (2.0309)	Prec@1 31.055 (23.107)	Prec@5 85.938 (75.874)	Acc 0.231 (0.190)	
TRAINING - Epoch: [0][140/781]	Time 0.137 (0.158)	Data 0.000 (0.017)	Loss 1.8608 (2.0209)	Prec@1 26.562 (23.375)	Prec@5 86.328 (76.443)	Acc 0.234 (0.193)	
TRAINING - Epoch: [0][150/781]	Time 0.137 (0.157)	Data 0.000 (0.016)	Loss 1.8402 (2.0140)	Prec@1 27.832 (23.651)	Prec@5 85.938 (76.841)	Acc 0.237 (0.196)	
TRAINING - Epoch: [0][160/781]	Time 0.137 (0.155)	Data 0.000 (0.015)	Loss 2.0065 (2.0041)	Prec@1 25.879 (23.958)	Prec@5 81.152 (77.326)	Acc 0.240 (0.198)	
TRAINING - Epoch: [0][170/781]	Time 0.137 (0.154)	Data 0.000 (0.014)	Loss 1.8897 (1.9961)	Prec@1 25.293 (24.183)	Prec@5 78.809 (77.703)	Acc 0.242 (0.201)	
TRAINING - Epoch: [0][180/781]	Time 0.137 (0.153)	Data 0.000 (0.013)	Loss 1.7470 (1.9840)	Prec@1 25.977 (24.587)	Prec@5 85.645 (78.173)	Acc 0.246 (0.203)	
TRAINING - Epoch: [0][190/781]	Time 0.137 (0.152)	Data 0.000 (0.012)	Loss 1.7439 (1.9731)	Prec@1 37.891 (24.961)	Prec@5 84.668 (78.632)	Acc 0.250 (0.206)	
TRAINING - Epoch: [0][200/781]	Time 0.137 (0.152)	Data 0.000 (0.012)	Loss 1.7694 (1.9640)	Prec@1 30.664 (25.162)	Prec@5 86.133 (78.996)	Acc 0.252 (0.208)	
TRAINING - Epoch: [0][210/781]	Time 0.137 (0.151)	Data 0.000 (0.011)	Loss 1.7044 (1.9552)	Prec@1 37.109 (25.435)	Prec@5 90.723 (79.349)	Acc 0.254 (0.210)	
TRAINING - Epoch: [0][220/781]	Time 0.137 (0.150)	Data 0.000 (0.011)	Loss 1.9097 (1.9475)	Prec@1 32.227 (25.795)	Prec@5 81.152 (79.658)	Acc 0.258 (0.212)	
TRAINING - Epoch: [0][230/781]	Time 0.137 (0.150)	Data 0.000 (0.010)	Loss 1.7845 (1.9395)	Prec@1 32.422 (26.127)	Prec@5 87.598 (79.989)	Acc 0.261 (0.214)	
TRAINING - Epoch: [0][240/781]	Time 0.137 (0.149)	Data 0.000 (0.010)	Loss 1.6353 (1.9293)	Prec@1 39.844 (26.523)	Prec@5 90.039 (80.336)	Acc 0.265 (0.216)	
TRAINING - Epoch: [0][250/781]	Time 0.137 (0.149)	Data 0.000 (0.009)	Loss 1.6632 (1.9207)	Prec@1 35.352 (26.876)	Prec@5 89.160 (80.646)	Acc 0.269 (0.218)	
TRAINING - Epoch: [0][260/781]	Time 0.137 (0.148)	Data 0.000 (0.009)	Loss 1.9062 (1.9154)	Prec@1 31.641 (27.042)	Prec@5 79.004 (80.836)	Acc 0.270 (0.220)	
TRAINING - Epoch: [0][270/781]	Time 0.137 (0.148)	Data 0.000 (0.009)	Loss 1.7965 (1.9092)	Prec@1 30.371 (27.249)	Prec@5 82.715 (81.080)	Acc 0.272 (0.222)	
TRAINING - Epoch: [0][280/781]	Time 0.137 (0.147)	Data 0.000 (0.008)	Loss 1.5771 (1.8999)	Prec@1 42.871 (27.625)	Prec@5 89.160 (81.360)	Acc 0.276 (0.224)	
TRAINING - Epoch: [0][290/781]	Time 0.137 (0.147)	Data 0.000 (0.008)	Loss 1.8732 (1.8925)	Prec@1 25.879 (27.883)	Prec@5 84.961 (81.623)	Acc 0.279 (0.226)	
TRAINING - Epoch: [0][300/781]	Time 0.137 (0.147)	Data 0.000 (0.008)	Loss 1.4869 (1.8850)	Prec@1 46.973 (28.184)	Prec@5 91.406 (81.826)	Acc 0.282 (0.228)	
TRAINING - Epoch: [0][310/781]	Time 0.137 (0.146)	Data 0.000 (0.008)	Loss 1.5855 (1.8776)	Prec@1 35.938 (28.522)	Prec@5 91.406 (82.016)	Acc 0.285 (0.229)	
TRAINING - Epoch: [0][320/781]	Time 0.137 (0.146)	Data 0.000 (0.007)	Loss 1.8189 (1.8722)	Prec@1 29.492 (28.750)	Prec@5 84.180 (82.198)	Acc 0.287 (0.231)	
TRAINING - Epoch: [0][330/781]	Time 0.137 (0.146)	Data 0.000 (0.007)	Loss 1.5502 (1.8651)	Prec@1 46.191 (29.007)	Prec@5 87.109 (82.416)	Acc 0.290 (0.233)	
TRAINING - Epoch: [0][340/781]	Time 0.137 (0.146)	Data 0.000 (0.007)	Loss 1.7819 (1.8572)	Prec@1 34.082 (29.356)	Prec@5 89.160 (82.627)	Acc 0.294 (0.235)	
TRAINING - Epoch: [0][350/781]	Time 0.137 (0.145)	Data 0.000 (0.007)	Loss 1.7960 (1.8508)	Prec@1 31.543 (29.645)	Prec@5 85.156 (82.792)	Acc 0.296 (0.236)	
TRAINING - Epoch: [0][360/781]	Time 0.137 (0.145)	Data 0.000 (0.007)	Loss 1.4928 (1.8439)	Prec@1 41.016 (29.928)	Prec@5 93.555 (83.014)	Acc 0.299 (0.238)	
TRAINING - Epoch: [0][370/781]	Time 0.137 (0.145)	Data 0.000 (0.006)	Loss 1.5127 (1.8377)	Prec@1 46.387 (30.209)	Prec@5 90.820 (83.183)	Acc 0.302 (0.240)	
TRAINING - Epoch: [0][380/781]	Time 0.137 (0.145)	Data 0.000 (0.006)	Loss 1.3767 (1.8308)	Prec@1 43.652 (30.489)	Prec@5 95.605 (83.369)	Acc 0.305 (0.241)	
TRAINING - Epoch: [0][390/781]	Time 0.137 (0.144)	Data 0.000 (0.006)	Loss 1.5327 (1.8238)	Prec@1 44.336 (30.762)	Prec@5 91.504 (83.560)	Acc 0.308 (0.243)	
TRAINING - Epoch: [0][400/781]	Time 0.137 (0.144)	Data 0.000 (0.006)	Loss 1.5765 (1.8176)	Prec@1 42.578 (31.061)	Prec@5 92.480 (83.716)	Acc 0.311 (0.245)	
TRAINING - Epoch: [0][410/781]	Time 0.137 (0.144)	Data 0.000 (0.006)	Loss 1.5732 (1.8120)	Prec@1 39.160 (31.298)	Prec@5 92.969 (83.865)	Acc 0.313 (0.246)	
TRAINING - Epoch: [0][420/781]	Time 0.137 (0.144)	Data 0.000 (0.006)	Loss 1.5809 (1.8068)	Prec@1 37.402 (31.521)	Prec@5 93.945 (84.004)	Acc 0.315 (0.248)	
TRAINING - Epoch: [0][430/781]	Time 0.137 (0.144)	Data 0.000 (0.006)	Loss 1.5374 (1.8000)	Prec@1 41.895 (31.801)	Prec@5 94.727 (84.165)	Acc 0.318 (0.250)	
TRAINING - Epoch: [0][440/781]	Time 0.137 (0.144)	Data 0.000 (0.005)	Loss 1.6098 (1.7934)	Prec@1 40.723 (32.097)	Prec@5 91.504 (84.327)	Acc 0.321 (0.251)	
TRAINING - Epoch: [0][450/781]	Time 0.137 (0.143)	Data 0.000 (0.005)	Loss 1.5740 (1.7884)	Prec@1 42.871 (32.295)	Prec@5 87.695 (84.452)	Acc 0.323 (0.253)	
TRAINING - Epoch: [0][460/781]	Time 0.137 (0.143)	Data 0.000 (0.005)	Loss 1.6528 (1.7835)	Prec@1 37.891 (32.507)	Prec@5 90.527 (84.565)	Acc 0.325 (0.254)	
TRAINING - Epoch: [0][470/781]	Time 0.137 (0.143)	Data 0.000 (0.005)	Loss 1.6253 (1.7772)	Prec@1 39.746 (32.773)	Prec@5 91.602 (84.716)	Acc 0.328 (0.256)	
TRAINING - Epoch: [0][480/781]	Time 0.137 (0.143)	Data 0.000 (0.005)	Loss 1.4495 (1.7731)	Prec@1 44.727 (32.947)	Prec@5 92.285 (84.848)	Acc 0.329 (0.257)	
TRAINING - Epoch: [0][490/781]	Time 0.137 (0.143)	Data 0.000 (0.005)	Loss 1.5049 (1.7676)	Prec@1 45.215 (33.164)	Prec@5 90.527 (84.997)	Acc 0.332 (0.259)	
TRAINING - Epoch: [0][500/781]	Time 0.137 (0.143)	Data 0.000 (0.005)	Loss 1.3474 (1.7622)	Prec@1 50.293 (33.364)	Prec@5 92.383 (85.152)	Acc 0.334 (0.260)	
TRAINING - Epoch: [0][510/781]	Time 0.137 (0.143)	Data 0.000 (0.005)	Loss 1.4597 (1.7578)	Prec@1 44.434 (33.547)	Prec@5 92.480 (85.276)	Acc 0.335 (0.262)	
TRAINING - Epoch: [0][520/781]	Time 0.137 (0.143)	Data 0.000 (0.005)	Loss 1.5865 (1.7542)	Prec@1 43.359 (33.699)	Prec@5 89.258 (85.385)	Acc 0.337 (0.263)	
TRAINING - Epoch: [0][530/781]	Time 0.137 (0.142)	Data 0.000 (0.005)	Loss 1.6185 (1.7501)	Prec@1 33.691 (33.866)	Prec@5 89.258 (85.494)	Acc 0.339 (0.265)	
TRAINING - Epoch: [0][540/781]	Time 0.139 (0.142)	Data 0.000 (0.004)	Loss 1.3672 (1.7446)	Prec@1 49.512 (34.100)	Prec@5 91.797 (85.622)	Acc 0.341 (0.266)	
TRAINING - Epoch: [0][550/781]	Time 0.137 (0.142)	Data 0.000 (0.004)	Loss 1.4709 (1.7398)	Prec@1 46.387 (34.296)	Prec@5 90.918 (85.751)	Acc 0.343 (0.267)	
TRAINING - Epoch: [0][560/781]	Time 0.137 (0.142)	Data 0.000 (0.004)	Loss 1.5403 (1.7349)	Prec@1 42.285 (34.494)	Prec@5 91.602 (85.859)	Acc 0.345 (0.269)	
TRAINING - Epoch: [0][570/781]	Time 0.137 (0.142)	Data 0.000 (0.004)	Loss 1.3580 (1.7289)	Prec@1 43.750 (34.732)	Prec@5 95.605 (85.991)	Acc 0.347 (0.270)	
TRAINING - Epoch: [0][580/781]	Time 0.137 (0.142)	Data 0.000 (0.004)	Loss 1.4222 (1.7234)	Prec@1 52.148 (34.933)	Prec@5 93.555 (86.125)	Acc 0.349 (0.271)	
TRAINING - Epoch: [0][590/781]	Time 0.137 (0.142)	Data 0.000 (0.004)	Loss 1.4399 (1.7179)	Prec@1 42.871 (35.157)	Prec@5 92.188 (86.252)	Acc 0.352 (0.273)	
TRAINING - Epoch: [0][600/781]	Time 0.137 (0.142)	Data 0.000 (0.004)	Loss 1.3646 (1.7125)	Prec@1 46.680 (35.385)	Prec@5 96.289 (86.364)	Acc 0.354 (0.274)	
TRAINING - Epoch: [0][610/781]	Time 0.137 (0.142)	Data 0.000 (0.004)	Loss 1.5543 (1.7074)	Prec@1 40.527 (35.565)	Prec@5 91.016 (86.478)	Acc 0.356 (0.275)	
TRAINING - Epoch: [0][620/781]	Time 0.137 (0.142)	Data 0.000 (0.004)	Loss 1.4308 (1.7035)	Prec@1 50.195 (35.722)	Prec@5 88.965 (86.552)	Acc 0.357 (0.277)	
TRAINING - Epoch: [0][630/781]	Time 0.137 (0.141)	Data 0.000 (0.004)	Loss 1.4062 (1.6995)	Prec@1 48.828 (35.884)	Prec@5 90.430 (86.644)	Acc 0.359 (0.278)	
TRAINING - Epoch: [0][640/781]	Time 0.137 (0.141)	Data 0.000 (0.004)	Loss 1.2381 (1.6943)	Prec@1 50.977 (36.089)	Prec@5 95.215 (86.749)	Acc 0.361 (0.279)	
TRAINING - Epoch: [0][650/781]	Time 0.137 (0.141)	Data 0.000 (0.004)	Loss 1.6058 (1.6887)	Prec@1 48.340 (36.352)	Prec@5 86.230 (86.853)	Acc 0.364 (0.281)	
TRAINING - Epoch: [0][660/781]	Time 0.137 (0.141)	Data 0.000 (0.004)	Loss 1.4906 (1.6843)	Prec@1 46.484 (36.520)	Prec@5 91.895 (86.957)	Acc 0.365 (0.282)	
TRAINING - Epoch: [0][670/781]	Time 0.137 (0.141)	Data 0.000 (0.004)	Loss 1.3909 (1.6799)	Prec@1 53.809 (36.706)	Prec@5 94.336 (87.041)	Acc 0.367 (0.283)	
TRAINING - Epoch: [0][680/781]	Time 0.137 (0.141)	Data 0.000 (0.004)	Loss 1.2868 (1.6747)	Prec@1 53.223 (36.914)	Prec@5 91.602 (87.142)	Acc 0.369 (0.284)	
TRAINING - Epoch: [0][690/781]	Time 0.137 (0.141)	Data 0.000 (0.004)	Loss 1.3381 (1.6701)	Prec@1 49.316 (37.092)	Prec@5 91.797 (87.234)	Acc 0.371 (0.286)	
TRAINING - Epoch: [0][700/781]	Time 0.137 (0.141)	Data 0.000 (0.003)	Loss 1.2800 (1.6652)	Prec@1 49.805 (37.292)	Prec@5 91.309 (87.329)	Acc 0.373 (0.287)	
TRAINING - Epoch: [0][710/781]	Time 0.137 (0.141)	Data 0.000 (0.003)	Loss 1.3642 (1.6610)	Prec@1 48.535 (37.460)	Prec@5 93.262 (87.413)	Acc 0.375 (0.288)	
TRAINING - Epoch: [0][720/781]	Time 0.137 (0.141)	Data 0.000 (0.003)	Loss 1.2840 (1.6572)	Prec@1 51.660 (37.616)	Prec@5 95.703 (87.501)	Acc 0.376 (0.289)	
TRAINING - Epoch: [0][730/781]	Time 0.137 (0.141)	Data 0.000 (0.003)	Loss 1.3753 (1.6531)	Prec@1 47.070 (37.786)	Prec@5 95.508 (87.584)	Acc 0.378 (0.290)	
TRAINING - Epoch: [0][740/781]	Time 0.137 (0.141)	Data 0.000 (0.003)	Loss 1.4105 (1.6487)	Prec@1 51.660 (37.978)	Prec@5 93.066 (87.668)	Acc 0.380 (0.292)	
TRAINING - Epoch: [0][750/781]	Time 0.137 (0.141)	Data 0.000 (0.003)	Loss 1.4152 (1.6445)	Prec@1 45.215 (38.147)	Prec@5 92.773 (87.751)	Acc 0.381 (0.293)	
TRAINING - Epoch: [0][760/781]	Time 0.137 (0.141)	Data 0.000 (0.003)	Loss 1.4209 (1.6404)	Prec@1 48.926 (38.332)	Prec@5 92.090 (87.834)	Acc 0.383 (0.294)	
TRAINING - Epoch: [0][770/781]	Time 0.137 (0.141)	Data 0.000 (0.003)	Loss 1.3085 (1.6365)	Prec@1 50.000 (38.491)	Prec@5 95.801 (87.925)	Acc 0.385 (0.295)	
TRAINING - Epoch: [0][780/781]	Time 0.137 (0.141)	Data 0.000 (0.003)	Loss 1.3076 (1.6327)	Prec@1 53.711 (38.664)	Prec@5 94.434 (87.995)	Acc 0.387 (0.296)	
Traceback (most recent call last):
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 364, in <module>
    main()
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 130, in main
    main_worker(args)
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 307, in main_worker
    train_results = trainer.train(train_data.get_loader(), chunk_batch=args.chunk_batch)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 282, in train
    return self.forward(data_loader, training=True, average_output=average_output, chunk_batch=chunk_batch)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 276, in forward
    return meter_results(meters)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 193, in meter_results
    results = {name: meter.avg for name, meter in meters.items()}
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 193, in <dictcomp>
    results = {name: meter.avg for name, meter in meters.items()}
AttributeError: 'float' object has no attribute 'avg'
saving to /scratch/tor213/DLS/question4/Hoffer/results/2022-03-31_05-10-44_resnet44_m-32
creating model resnet
created model with configuration: {'dataset': 'cifar10', 'depth': 44}
number of parameters: 661338
/scratch/tor213/.env/lib64/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
optimization regime: [{'epoch': 0, 'optimizer': 'SGD', 'lr': 0.1, 'momentum': 0.9, 'regularizer': {'name': 'WeightDecay', 'value': 0.0001, 'log': False, 'filter': {'parameter_name': <function weight_decay_config.<locals>.<lambda> at 0x15510c792af0>, 'module': <function weight_decay_config.<locals>.<lambda> at 0x15510c792940>}}}, {'epoch': 81, 'lr': 0.01}, {'epoch': 122, 'lr': 0.001}, {'epoch': 164, 'lr': 0.0001}]
data regime: Current: {'datasets_path': '~/Datasets', 'name': 'cifar10', 'split': 'train', 'augment': True, 'input_size': None, 'batch_size': 64, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'drop_last': True, 'distributed': False, 'duplicates': 32, 'autoaugment': False, 'cutout': {'holes': 1, 'length': 16}}
 Regime:None

Starting Epoch: 1

Files already downloaded and verified
Files already downloaded and verified
TRAINING - Epoch: [0][0/781]	Time 4.514 (4.514)	Data 2.980 (2.980)	Loss 2.3066 (2.3066)	Prec@1 6.738 (6.738)	Prec@5 41.748 (41.748)	Acc 0.067 (0.067)	
TRAINING - Epoch: [0][10/781]	Time 0.266 (0.654)	Data 0.000 (0.271)	Loss 2.3284 (2.3035)	Prec@1 10.449 (13.450)	Prec@5 54.395 (55.562)	Acc 0.134 (0.117)	
TRAINING - Epoch: [0][20/781]	Time 0.266 (0.469)	Data 0.000 (0.142)	Loss 2.2577 (2.2598)	Prec@1 11.133 (14.983)	Prec@5 63.281 (59.763)	Acc 0.150 (0.131)	
TRAINING - Epoch: [0][30/781]	Time 0.266 (0.404)	Data 0.000 (0.096)	Loss 2.1469 (2.2293)	Prec@1 25.488 (16.561)	Prec@5 67.676 (62.591)	Acc 0.166 (0.140)	
TRAINING - Epoch: [0][40/781]	Time 0.267 (0.370)	Data 0.000 (0.073)	Loss 2.1914 (2.1881)	Prec@1 20.605 (17.683)	Prec@5 75.684 (66.029)	Acc 0.177 (0.148)	
TRAINING - Epoch: [0][50/781]	Time 0.266 (0.350)	Data 0.000 (0.059)	Loss 1.9222 (2.1593)	Prec@1 27.393 (18.759)	Prec@5 81.006 (68.130)	Acc 0.188 (0.155)	
TRAINING - Epoch: [0][60/781]	Time 0.267 (0.336)	Data 0.000 (0.049)	Loss 1.9404 (2.1335)	Prec@1 20.166 (19.380)	Prec@5 82.861 (69.938)	Acc 0.194 (0.161)	
TRAINING - Epoch: [0][70/781]	Time 0.267 (0.327)	Data 0.000 (0.042)	Loss 2.0331 (2.1192)	Prec@1 25.439 (19.698)	Prec@5 79.932 (71.119)	Acc 0.197 (0.166)	
TRAINING - Epoch: [0][80/781]	Time 0.267 (0.319)	Data 0.000 (0.037)	Loss 1.9869 (2.1038)	Prec@1 25.537 (20.241)	Prec@5 76.416 (72.193)	Acc 0.202 (0.170)	
TRAINING - Epoch: [0][90/781]	Time 0.267 (0.313)	Data 0.000 (0.033)	Loss 2.0066 (2.0866)	Prec@1 21.826 (20.794)	Prec@5 77.344 (73.283)	Acc 0.208 (0.174)	
TRAINING - Epoch: [0][100/781]	Time 0.267 (0.309)	Data 0.000 (0.030)	Loss 1.8969 (2.0701)	Prec@1 28.564 (21.612)	Prec@5 78.613 (74.079)	Acc 0.216 (0.178)	
TRAINING - Epoch: [0][110/781]	Time 0.266 (0.305)	Data 0.000 (0.027)	Loss 1.8881 (2.0581)	Prec@1 30.371 (22.180)	Prec@5 80.664 (74.622)	Acc 0.222 (0.181)	
TRAINING - Epoch: [0][120/781]	Time 0.267 (0.302)	Data 0.000 (0.025)	Loss 1.8374 (2.0451)	Prec@1 27.783 (22.576)	Prec@5 83.984 (75.253)	Acc 0.226 (0.185)	
TRAINING - Epoch: [0][130/781]	Time 0.267 (0.299)	Data 0.000 (0.023)	Loss 1.9233 (2.0314)	Prec@1 28.223 (23.092)	Prec@5 82.568 (75.882)	Acc 0.231 (0.188)	
TRAINING - Epoch: [0][140/781]	Time 0.267 (0.297)	Data 0.000 (0.021)	Loss 1.8406 (2.0195)	Prec@1 29.932 (23.508)	Prec@5 85.010 (76.479)	Acc 0.235 (0.191)	
TRAINING - Epoch: [0][150/781]	Time 0.267 (0.295)	Data 0.000 (0.020)	Loss 1.8182 (2.0119)	Prec@1 29.346 (23.809)	Prec@5 87.109 (76.891)	Acc 0.238 (0.194)	
TRAINING - Epoch: [0][160/781]	Time 0.267 (0.293)	Data 0.000 (0.019)	Loss 1.9683 (2.0018)	Prec@1 26.172 (24.092)	Prec@5 81.445 (77.332)	Acc 0.241 (0.197)	
TRAINING - Epoch: [0][170/781]	Time 0.266 (0.292)	Data 0.000 (0.018)	Loss 1.8611 (1.9938)	Prec@1 25.928 (24.362)	Prec@5 81.152 (77.747)	Acc 0.244 (0.200)	
TRAINING - Epoch: [0][180/781]	Time 0.267 (0.290)	Data 0.000 (0.017)	Loss 1.7698 (1.9828)	Prec@1 24.561 (24.740)	Prec@5 85.205 (78.172)	Acc 0.247 (0.202)	
TRAINING - Epoch: [0][190/781]	Time 0.267 (0.289)	Data 0.000 (0.016)	Loss 1.7924 (1.9733)	Prec@1 36.670 (25.064)	Prec@5 84.863 (78.616)	Acc 0.251 (0.205)	
TRAINING - Epoch: [0][200/781]	Time 0.267 (0.288)	Data 0.000 (0.015)	Loss 1.8608 (1.9646)	Prec@1 28.418 (25.237)	Prec@5 83.447 (78.994)	Acc 0.252 (0.207)	
TRAINING - Epoch: [0][210/781]	Time 0.266 (0.287)	Data 0.000 (0.014)	Loss 1.6989 (1.9555)	Prec@1 35.840 (25.519)	Prec@5 92.041 (79.372)	Acc 0.255 (0.209)	
TRAINING - Epoch: [0][220/781]	Time 0.267 (0.286)	Data 0.000 (0.014)	Loss 1.9073 (1.9487)	Prec@1 31.885 (25.838)	Prec@5 82.910 (79.661)	Acc 0.258 (0.212)	
TRAINING - Epoch: [0][230/781]	Time 0.266 (0.285)	Data 0.000 (0.013)	Loss 1.8018 (1.9415)	Prec@1 30.957 (26.107)	Prec@5 85.938 (79.988)	Acc 0.261 (0.214)	
TRAINING - Epoch: [0][240/781]	Time 0.267 (0.284)	Data 0.000 (0.013)	Loss 1.5940 (1.9313)	Prec@1 41.260 (26.507)	Prec@5 90.332 (80.358)	Acc 0.265 (0.216)	
TRAINING - Epoch: [0][250/781]	Time 0.266 (0.284)	Data 0.000 (0.012)	Loss 1.6876 (1.9220)	Prec@1 37.061 (26.871)	Prec@5 89.014 (80.699)	Acc 0.269 (0.218)	
TRAINING - Epoch: [0][260/781]	Time 0.267 (0.283)	Data 0.000 (0.012)	Loss 1.8834 (1.9162)	Prec@1 31.055 (27.034)	Prec@5 81.152 (80.893)	Acc 0.270 (0.220)	
TRAINING - Epoch: [0][270/781]	Time 0.267 (0.282)	Data 0.000 (0.011)	Loss 1.7665 (1.9096)	Prec@1 33.301 (27.297)	Prec@5 83.154 (81.129)	Acc 0.273 (0.222)	
TRAINING - Epoch: [0][280/781]	Time 0.267 (0.282)	Data 0.000 (0.011)	Loss 1.5822 (1.9004)	Prec@1 44.580 (27.680)	Prec@5 88.574 (81.363)	Acc 0.277 (0.224)	
TRAINING - Epoch: [0][290/781]	Time 0.266 (0.281)	Data 0.000 (0.010)	Loss 1.8298 (1.8931)	Prec@1 26.562 (27.976)	Prec@5 85.742 (81.617)	Acc 0.280 (0.225)	
TRAINING - Epoch: [0][300/781]	Time 0.266 (0.281)	Data 0.000 (0.010)	Loss 1.4823 (1.8844)	Prec@1 45.166 (28.288)	Prec@5 92.432 (81.863)	Acc 0.283 (0.227)	
TRAINING - Epoch: [0][310/781]	Time 0.267 (0.280)	Data 0.000 (0.010)	Loss 1.5584 (1.8770)	Prec@1 39.941 (28.606)	Prec@5 93.652 (82.054)	Acc 0.286 (0.229)	
TRAINING - Epoch: [0][320/781]	Time 0.267 (0.280)	Data 0.000 (0.009)	Loss 1.7809 (1.8717)	Prec@1 28.320 (28.813)	Prec@5 84.424 (82.224)	Acc 0.288 (0.231)	
TRAINING - Epoch: [0][330/781]	Time 0.267 (0.279)	Data 0.000 (0.009)	Loss 1.5354 (1.8647)	Prec@1 49.707 (29.074)	Prec@5 88.623 (82.434)	Acc 0.291 (0.233)	
TRAINING - Epoch: [0][340/781]	Time 0.266 (0.279)	Data 0.000 (0.009)	Loss 1.7125 (1.8571)	Prec@1 37.695 (29.439)	Prec@5 90.137 (82.636)	Acc 0.294 (0.234)	
TRAINING - Epoch: [0][350/781]	Time 0.267 (0.279)	Data 0.000 (0.009)	Loss 1.7582 (1.8507)	Prec@1 31.250 (29.717)	Prec@5 88.086 (82.799)	Acc 0.297 (0.236)	
TRAINING - Epoch: [0][360/781]	Time 0.267 (0.278)	Data 0.000 (0.008)	Loss 1.4788 (1.8436)	Prec@1 43.750 (30.023)	Prec@5 93.018 (83.016)	Acc 0.300 (0.238)	
TRAINING - Epoch: [0][370/781]	Time 0.266 (0.278)	Data 0.000 (0.008)	Loss 1.5643 (1.8374)	Prec@1 42.383 (30.286)	Prec@5 89.453 (83.187)	Acc 0.303 (0.240)	
TRAINING - Epoch: [0][380/781]	Time 0.266 (0.278)	Data 0.000 (0.008)	Loss 1.4697 (1.8308)	Prec@1 45.850 (30.562)	Prec@5 96.191 (83.365)	Acc 0.306 (0.241)	
TRAINING - Epoch: [0][390/781]	Time 0.267 (0.278)	Data 0.000 (0.008)	Loss 1.5882 (1.8246)	Prec@1 42.578 (30.822)	Prec@5 90.234 (83.537)	Acc 0.308 (0.243)	
TRAINING - Epoch: [0][400/781]	Time 0.266 (0.277)	Data 0.000 (0.008)	Loss 1.7032 (1.8189)	Prec@1 37.842 (31.093)	Prec@5 92.041 (83.692)	Acc 0.311 (0.245)	
TRAINING - Epoch: [0][410/781]	Time 0.266 (0.277)	Data 0.000 (0.007)	Loss 1.6399 (1.8135)	Prec@1 35.303 (31.305)	Prec@5 91.992 (83.831)	Acc 0.313 (0.246)	
TRAINING - Epoch: [0][420/781]	Time 0.267 (0.277)	Data 0.000 (0.007)	Loss 1.6300 (1.8080)	Prec@1 36.670 (31.545)	Prec@5 93.506 (83.980)	Acc 0.315 (0.248)	
TRAINING - Epoch: [0][430/781]	Time 0.266 (0.276)	Data 0.000 (0.007)	Loss 1.5140 (1.8014)	Prec@1 43.213 (31.831)	Prec@5 95.166 (84.138)	Acc 0.318 (0.250)	
TRAINING - Epoch: [0][440/781]	Time 0.266 (0.276)	Data 0.000 (0.007)	Loss 1.6613 (1.7942)	Prec@1 42.432 (32.140)	Prec@5 91.357 (84.330)	Acc 0.321 (0.251)	
TRAINING - Epoch: [0][450/781]	Time 0.266 (0.276)	Data 0.000 (0.007)	Loss 1.5896 (1.7892)	Prec@1 43.311 (32.338)	Prec@5 88.770 (84.455)	Acc 0.323 (0.253)	
TRAINING - Epoch: [0][460/781]	Time 0.267 (0.276)	Data 0.000 (0.007)	Loss 1.6335 (1.7846)	Prec@1 39.600 (32.548)	Prec@5 89.502 (84.556)	Acc 0.325 (0.254)	
TRAINING - Epoch: [0][470/781]	Time 0.266 (0.276)	Data 0.000 (0.007)	Loss 1.6772 (1.7777)	Prec@1 35.010 (32.811)	Prec@5 90.869 (84.712)	Acc 0.328 (0.256)	
TRAINING - Epoch: [0][480/781]	Time 0.267 (0.275)	Data 0.000 (0.006)	Loss 1.3817 (1.7735)	Prec@1 45.801 (33.003)	Prec@5 93.555 (84.830)	Acc 0.330 (0.257)	
TRAINING - Epoch: [0][490/781]	Time 0.266 (0.275)	Data 0.000 (0.006)	Loss 1.4421 (1.7677)	Prec@1 47.266 (33.240)	Prec@5 92.334 (84.984)	Acc 0.332 (0.259)	
TRAINING - Epoch: [0][500/781]	Time 0.267 (0.275)	Data 0.000 (0.006)	Loss 1.2999 (1.7619)	Prec@1 49.609 (33.473)	Prec@5 93.994 (85.135)	Acc 0.335 (0.260)	
TRAINING - Epoch: [0][510/781]	Time 0.266 (0.275)	Data 0.000 (0.006)	Loss 1.4732 (1.7573)	Prec@1 44.971 (33.663)	Prec@5 92.236 (85.269)	Acc 0.337 (0.262)	
TRAINING - Epoch: [0][520/781]	Time 0.266 (0.275)	Data 0.000 (0.006)	Loss 1.5152 (1.7536)	Prec@1 46.484 (33.813)	Prec@5 90.869 (85.364)	Acc 0.338 (0.263)	
TRAINING - Epoch: [0][530/781]	Time 0.267 (0.275)	Data 0.000 (0.006)	Loss 1.5381 (1.7489)	Prec@1 38.721 (34.003)	Prec@5 92.578 (85.485)	Acc 0.340 (0.265)	
TRAINING - Epoch: [0][540/781]	Time 0.266 (0.274)	Data 0.000 (0.006)	Loss 1.3323 (1.7428)	Prec@1 51.758 (34.278)	Prec@5 91.992 (85.612)	Acc 0.343 (0.266)	
TRAINING - Epoch: [0][550/781]	Time 0.266 (0.274)	Data 0.000 (0.006)	Loss 1.4104 (1.7376)	Prec@1 50.635 (34.513)	Prec@5 92.627 (85.737)	Acc 0.345 (0.268)	
TRAINING - Epoch: [0][560/781]	Time 0.266 (0.274)	Data 0.000 (0.006)	Loss 1.4654 (1.7322)	Prec@1 46.533 (34.739)	Prec@5 94.482 (85.856)	Acc 0.347 (0.269)	
TRAINING - Epoch: [0][570/781]	Time 0.266 (0.274)	Data 0.000 (0.005)	Loss 1.4452 (1.7263)	Prec@1 43.213 (34.979)	Prec@5 94.092 (85.984)	Acc 0.350 (0.270)	
TRAINING - Epoch: [0][580/781]	Time 0.267 (0.274)	Data 0.000 (0.005)	Loss 1.4172 (1.7208)	Prec@1 51.367 (35.193)	Prec@5 93.311 (86.124)	Acc 0.352 (0.272)	
TRAINING - Epoch: [0][590/781]	Time 0.267 (0.274)	Data 0.000 (0.005)	Loss 1.3725 (1.7150)	Prec@1 47.119 (35.421)	Prec@5 94.092 (86.255)	Acc 0.354 (0.273)	
TRAINING - Epoch: [0][600/781]	Time 0.266 (0.274)	Data 0.000 (0.005)	Loss 1.3485 (1.7099)	Prec@1 50.195 (35.640)	Prec@5 95.508 (86.367)	Acc 0.356 (0.274)	
TRAINING - Epoch: [0][610/781]	Time 0.267 (0.274)	Data 0.000 (0.005)	Loss 1.4729 (1.7045)	Prec@1 46.289 (35.835)	Prec@5 92.139 (86.491)	Acc 0.358 (0.276)	
TRAINING - Epoch: [0][620/781]	Time 0.267 (0.273)	Data 0.000 (0.005)	Loss 1.4450 (1.7010)	Prec@1 47.510 (35.973)	Prec@5 90.479 (86.564)	Acc 0.360 (0.277)	
TRAINING - Epoch: [0][630/781]	Time 0.267 (0.273)	Data 0.000 (0.005)	Loss 1.4117 (1.6967)	Prec@1 48.682 (36.160)	Prec@5 92.383 (86.653)	Acc 0.362 (0.279)	
TRAINING - Epoch: [0][640/781]	Time 0.266 (0.273)	Data 0.000 (0.005)	Loss 1.1939 (1.6916)	Prec@1 54.102 (36.367)	Prec@5 97.217 (86.762)	Acc 0.364 (0.280)	
TRAINING - Epoch: [0][650/781]	Time 0.267 (0.273)	Data 0.000 (0.005)	Loss 1.5445 (1.6857)	Prec@1 48.828 (36.630)	Prec@5 87.793 (86.871)	Acc 0.366 (0.281)	
TRAINING - Epoch: [0][660/781]	Time 0.266 (0.273)	Data 0.000 (0.005)	Loss 1.4890 (1.6812)	Prec@1 44.385 (36.817)	Prec@5 93.115 (86.968)	Acc 0.368 (0.282)	
TRAINING - Epoch: [0][670/781]	Time 0.267 (0.273)	Data 0.000 (0.005)	Loss 1.4815 (1.6770)	Prec@1 48.535 (36.981)	Prec@5 92.041 (87.056)	Acc 0.370 (0.284)	
TRAINING - Epoch: [0][680/781]	Time 0.267 (0.273)	Data 0.000 (0.005)	Loss 1.2782 (1.6722)	Prec@1 51.270 (37.170)	Prec@5 93.359 (87.154)	Acc 0.372 (0.285)	
TRAINING - Epoch: [0][690/781]	Time 0.266 (0.273)	Data 0.000 (0.005)	Loss 1.2668 (1.6679)	Prec@1 53.369 (37.331)	Prec@5 93.018 (87.249)	Acc 0.373 (0.286)	
TRAINING - Epoch: [0][700/781]	Time 0.266 (0.273)	Data 0.000 (0.004)	Loss 1.3155 (1.6629)	Prec@1 50.732 (37.536)	Prec@5 91.699 (87.342)	Acc 0.375 (0.288)	
TRAINING - Epoch: [0][710/781]	Time 0.266 (0.273)	Data 0.000 (0.004)	Loss 1.3030 (1.6586)	Prec@1 51.318 (37.709)	Prec@5 93.262 (87.429)	Acc 0.377 (0.289)	
TRAINING - Epoch: [0][720/781]	Time 0.266 (0.273)	Data 0.000 (0.004)	Loss 1.2721 (1.6546)	Prec@1 53.711 (37.889)	Prec@5 94.336 (87.507)	Acc 0.379 (0.290)	
TRAINING - Epoch: [0][730/781]	Time 0.266 (0.272)	Data 0.000 (0.004)	Loss 1.2804 (1.6501)	Prec@1 52.979 (38.066)	Prec@5 96.777 (87.592)	Acc 0.381 (0.291)	
TRAINING - Epoch: [0][740/781]	Time 0.266 (0.272)	Data 0.000 (0.004)	Loss 1.3741 (1.6454)	Prec@1 51.660 (38.267)	Prec@5 92.188 (87.676)	Acc 0.383 (0.292)	
TRAINING - Epoch: [0][750/781]	Time 0.266 (0.272)	Data 0.000 (0.004)	Loss 1.4826 (1.6413)	Prec@1 46.680 (38.432)	Prec@5 91.357 (87.759)	Acc 0.384 (0.294)	
TRAINING - Epoch: [0][760/781]	Time 0.267 (0.272)	Data 0.000 (0.004)	Loss 1.4155 (1.6371)	Prec@1 49.414 (38.616)	Prec@5 92.529 (87.843)	Acc 0.386 (0.295)	
TRAINING - Epoch: [0][770/781]	Time 0.267 (0.272)	Data 0.000 (0.004)	Loss 1.2837 (1.6329)	Prec@1 48.047 (38.772)	Prec@5 96.777 (87.940)	Acc 0.388 (0.296)	
TRAINING - Epoch: [0][780/781]	Time 0.266 (0.272)	Data 0.000 (0.004)	Loss 1.2223 (1.6292)	Prec@1 55.664 (38.945)	Prec@5 94.336 (88.005)	Acc 0.389 (0.297)	
Traceback (most recent call last):
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 364, in <module>
    main()
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 130, in main
    main_worker(args)
  File "/scratch/tor213/DLS/question4/Hoffer/main.py", line 307, in main_worker
    train_results = trainer.train(train_data.get_loader(), chunk_batch=args.chunk_batch)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 282, in train
    return self.forward(data_loader, training=True, average_output=average_output, chunk_batch=chunk_batch)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 276, in forward
    return meter_results(meters)
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 193, in meter_results
    results = {name: meter.avg for name, meter in meters.items()}
  File "/scratch/tor213/DLS/question4/Hoffer/trainer.py", line 193, in <dictcomp>
    results = {name: meter.avg for name, meter in meters.items()}
AttributeError: 'float' object has no attribute 'avg'
