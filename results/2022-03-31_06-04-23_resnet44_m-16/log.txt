2022-03-31 06:04:23,177 - INFO - saving to results/2022-03-31_06-04-23_resnet44_m-16
2022-03-31 06:04:23,179 - DEBUG - run arguments: Namespace(adapt_grad_norm=None, autoaugment=False, batch_size=64, chunk_batch=1, config_file=None, cutmix=None, cutout=True, dataset='cifar10', datasets_dir='~/Datasets', device='cuda', device_ids=[0], dist_backend='nccl', dist_init='env://', distributed=False, drop_optim_state=False, dtype='float', duplicates=16, epochs=100, eval_batch_size=-1, evaluate=None, grad_clip=-1, input_size=None, label_smoothing=0, local_rank=-1, loss_scale=1, lr=0.1, mixup=None, model='resnet', model_config="{'depth': 44}", momentum=0.9, optimizer='SGD', print_freq=10, results_dir='results/', resume='', save='2022-03-31_06-04-23', save_all=False, seed=123, start_epoch=-1, sync_bn=False, tensorwatch=False, tensorwatch_port=0, weight_decay=0, workers=8, world_size=-1)
2022-03-31 06:04:23,179 - INFO - creating model resnet
2022-03-31 06:04:23,239 - INFO - created model with configuration: {'dataset': 'cifar10', 'depth': 44}
2022-03-31 06:04:23,240 - INFO - number of parameters: 661338
2022-03-31 06:04:28,573 - INFO - optimization regime: [{'epoch': 0, 'optimizer': 'SGD', 'lr': 0.1, 'momentum': 0.9, 'regularizer': {'name': 'WeightDecay', 'value': 0.0001, 'log': False, 'filter': {'parameter_name': <function weight_decay_config.<locals>.<lambda> at 0x151e0b958a60>, 'module': <function weight_decay_config.<locals>.<lambda> at 0x151e0b958820>}}}, {'epoch': 81, 'lr': 0.01}, {'epoch': 122, 'lr': 0.001}, {'epoch': 164, 'lr': 0.0001}]
2022-03-31 06:04:28,575 - INFO - data regime: Current: {'datasets_path': '~/Datasets', 'name': 'cifar10', 'split': 'train', 'augment': True, 'input_size': None, 'batch_size': 64, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'drop_last': True, 'distributed': False, 'duplicates': 16, 'autoaugment': False, 'cutout': {'holes': 1, 'length': 16}}
 Regime:None
2022-03-31 06:04:28,575 - INFO - 
Starting Epoch: 1

2022-03-31 06:04:30,823 - DEBUG - OPTIMIZER - setting lr = 0.1
2022-03-31 06:04:30,823 - DEBUG - OPTIMIZER - setting momentum = 0.9
2022-03-31 06:04:31,979 - INFO - TRAINING - Epoch: [0][0/781]	Time 3.403 (3.403)	Data 2.246 (2.246)	Loss 2.3063 (2.3063)	Prec@1 8.203 (8.203)	Prec@5 41.309 (41.309)	Acc 0.082 (0.082)	
2022-03-31 06:04:33,764 - INFO - TRAINING - Epoch: [0][10/781]	Time 0.178 (0.472)	Data 0.000 (0.205)	Loss 2.3270 (2.3034)	Prec@1 10.938 (13.707)	Prec@5 53.906 (55.513)	Acc 0.137 (0.122)	
2022-03-31 06:04:35,548 - INFO - TRAINING - Epoch: [0][20/781]	Time 0.178 (0.332)	Data 0.000 (0.107)	Loss 2.2528 (2.2592)	Prec@1 10.742 (15.332)	Prec@5 64.844 (59.761)	Acc 0.153 (0.136)	
2022-03-31 06:04:37,331 - INFO - TRAINING - Epoch: [0][30/781]	Time 0.178 (0.282)	Data 0.000 (0.073)	Loss 2.1509 (2.2293)	Prec@1 26.172 (16.791)	Prec@5 67.090 (62.579)	Acc 0.168 (0.144)	
2022-03-31 06:04:39,116 - INFO - TRAINING - Epoch: [0][40/781]	Time 0.178 (0.257)	Data 0.000 (0.055)	Loss 2.1898 (2.1883)	Prec@1 20.215 (17.731)	Prec@5 75.293 (66.118)	Acc 0.177 (0.151)	
2022-03-31 06:04:40,900 - INFO - TRAINING - Epoch: [0][50/781]	Time 0.178 (0.242)	Data 0.000 (0.044)	Loss 1.9108 (2.1584)	Prec@1 28.223 (18.830)	Prec@5 80.273 (68.246)	Acc 0.188 (0.158)	
2022-03-31 06:04:42,683 - INFO - TRAINING - Epoch: [0][60/781]	Time 0.178 (0.231)	Data 0.000 (0.037)	Loss 1.9344 (2.1326)	Prec@1 22.168 (19.568)	Prec@5 83.496 (69.968)	Acc 0.196 (0.163)	
2022-03-31 06:04:44,558 - INFO - TRAINING - Epoch: [0][70/781]	Time 0.178 (0.225)	Data 0.000 (0.033)	Loss 2.0178 (2.1188)	Prec@1 26.270 (19.874)	Prec@5 80.762 (71.165)	Acc 0.199 (0.168)	
2022-03-31 06:04:46,341 - INFO - TRAINING - Epoch: [0][80/781]	Time 0.178 (0.219)	Data 0.000 (0.029)	Loss 2.0083 (2.1036)	Prec@1 25.293 (20.427)	Prec@5 73.926 (72.204)	Acc 0.204 (0.172)	
2022-03-31 06:04:48,126 - INFO - TRAINING - Epoch: [0][90/781]	Time 0.178 (0.215)	Data 0.000 (0.026)	Loss 2.0040 (2.0870)	Prec@1 23.730 (20.998)	Prec@5 77.051 (73.269)	Acc 0.210 (0.176)	
2022-03-31 06:04:49,916 - INFO - TRAINING - Epoch: [0][100/781]	Time 0.178 (0.211)	Data 0.000 (0.024)	Loss 1.9248 (2.0711)	Prec@1 29.395 (21.757)	Prec@5 78.027 (74.063)	Acc 0.218 (0.180)	
2022-03-31 06:04:51,701 - INFO - TRAINING - Epoch: [0][110/781]	Time 0.178 (0.208)	Data 0.000 (0.021)	Loss 1.8627 (2.0578)	Prec@1 29.492 (22.248)	Prec@5 80.957 (74.601)	Acc 0.222 (0.183)	
2022-03-31 06:04:53,487 - INFO - TRAINING - Epoch: [0][120/781]	Time 0.179 (0.206)	Data 0.000 (0.020)	Loss 1.8270 (2.0449)	Prec@1 27.246 (22.599)	Prec@5 84.570 (75.233)	Acc 0.226 (0.187)	
2022-03-31 06:04:55,273 - INFO - TRAINING - Epoch: [0][130/781]	Time 0.178 (0.204)	Data 0.000 (0.018)	Loss 1.8709 (2.0305)	Prec@1 31.641 (23.127)	Prec@5 84.277 (75.877)	Acc 0.231 (0.190)	
2022-03-31 06:04:57,060 - INFO - TRAINING - Epoch: [0][140/781]	Time 0.179 (0.202)	Data 0.000 (0.017)	Loss 1.8766 (2.0211)	Prec@1 28.613 (23.379)	Prec@5 85.938 (76.420)	Acc 0.234 (0.193)	
2022-03-31 06:04:58,851 - INFO - TRAINING - Epoch: [0][150/781]	Time 0.179 (0.200)	Data 0.000 (0.016)	Loss 1.8428 (2.0138)	Prec@1 28.906 (23.630)	Prec@5 86.133 (76.839)	Acc 0.236 (0.196)	
2022-03-31 06:05:00,641 - INFO - TRAINING - Epoch: [0][160/781]	Time 0.179 (0.199)	Data 0.000 (0.015)	Loss 2.0218 (2.0035)	Prec@1 24.805 (24.011)	Prec@5 80.957 (77.343)	Acc 0.240 (0.198)	
2022-03-31 06:05:02,518 - INFO - TRAINING - Epoch: [0][170/781]	Time 0.179 (0.198)	Data 0.000 (0.015)	Loss 1.9195 (1.9957)	Prec@1 24.512 (24.238)	Prec@5 76.953 (77.706)	Acc 0.242 (0.201)	
2022-03-31 06:05:04,307 - INFO - TRAINING - Epoch: [0][180/781]	Time 0.179 (0.197)	Data 0.000 (0.014)	Loss 1.7344 (1.9837)	Prec@1 25.098 (24.651)	Prec@5 87.207 (78.173)	Acc 0.247 (0.203)	
2022-03-31 06:05:06,095 - INFO - TRAINING - Epoch: [0][190/781]	Time 0.178 (0.196)	Data 0.000 (0.013)	Loss 1.7571 (1.9730)	Prec@1 38.574 (25.018)	Prec@5 82.812 (78.599)	Acc 0.250 (0.206)	
2022-03-31 06:05:07,884 - INFO - TRAINING - Epoch: [0][200/781]	Time 0.178 (0.196)	Data 0.000 (0.012)	Loss 1.7703 (1.9640)	Prec@1 34.180 (25.200)	Prec@5 84.961 (78.943)	Acc 0.252 (0.208)	
2022-03-31 06:05:09,672 - INFO - TRAINING - Epoch: [0][210/781]	Time 0.179 (0.195)	Data 0.000 (0.012)	Loss 1.6962 (1.9551)	Prec@1 36.719 (25.486)	Prec@5 90.234 (79.315)	Acc 0.255 (0.210)	
2022-03-31 06:05:11,460 - INFO - TRAINING - Epoch: [0][220/781]	Time 0.179 (0.194)	Data 0.000 (0.011)	Loss 1.9190 (1.9478)	Prec@1 31.348 (25.848)	Prec@5 81.641 (79.622)	Acc 0.258 (0.212)	
2022-03-31 06:05:13,249 - INFO - TRAINING - Epoch: [0][230/781]	Time 0.179 (0.193)	Data 0.000 (0.011)	Loss 1.7816 (1.9400)	Prec@1 33.594 (26.135)	Prec@5 87.207 (79.942)	Acc 0.261 (0.214)	
2022-03-31 06:05:15,036 - INFO - TRAINING - Epoch: [0][240/781]	Time 0.179 (0.193)	Data 0.000 (0.010)	Loss 1.6629 (1.9305)	Prec@1 37.695 (26.496)	Prec@5 89.941 (80.263)	Acc 0.265 (0.216)	
2022-03-31 06:05:16,824 - INFO - TRAINING - Epoch: [0][250/781]	Time 0.179 (0.192)	Data 0.000 (0.010)	Loss 1.6811 (1.9221)	Prec@1 36.328 (26.875)	Prec@5 88.281 (80.562)	Acc 0.269 (0.218)	
2022-03-31 06:05:18,611 - INFO - TRAINING - Epoch: [0][260/781]	Time 0.179 (0.192)	Data 0.000 (0.010)	Loss 1.9407 (1.9171)	Prec@1 31.348 (27.048)	Prec@5 79.883 (80.752)	Acc 0.270 (0.220)	
2022-03-31 06:05:20,400 - INFO - TRAINING - Epoch: [0][270/781]	Time 0.179 (0.191)	Data 0.000 (0.009)	Loss 1.7780 (1.9111)	Prec@1 32.227 (27.241)	Prec@5 83.594 (80.997)	Acc 0.272 (0.222)	
2022-03-31 06:05:22,188 - INFO - TRAINING - Epoch: [0][280/781]	Time 0.179 (0.191)	Data 0.000 (0.009)	Loss 1.5256 (1.9016)	Prec@1 45.117 (27.619)	Prec@5 92.871 (81.287)	Acc 0.276 (0.224)	
2022-03-31 06:05:23,975 - INFO - TRAINING - Epoch: [0][290/781]	Time 0.179 (0.190)	Data 0.000 (0.009)	Loss 1.8746 (1.8942)	Prec@1 26.074 (27.885)	Prec@5 85.645 (81.546)	Acc 0.279 (0.226)	
2022-03-31 06:05:25,763 - INFO - TRAINING - Epoch: [0][300/781]	Time 0.179 (0.190)	Data 0.000 (0.008)	Loss 1.5024 (1.8867)	Prec@1 44.238 (28.146)	Prec@5 91.113 (81.757)	Acc 0.281 (0.228)	
2022-03-31 06:05:27,552 - INFO - TRAINING - Epoch: [0][310/781]	Time 0.178 (0.190)	Data 0.000 (0.008)	Loss 1.6391 (1.8795)	Prec@1 32.324 (28.468)	Prec@5 91.211 (81.952)	Acc 0.285 (0.230)	
2022-03-31 06:05:29,340 - INFO - TRAINING - Epoch: [0][320/781]	Time 0.179 (0.189)	Data 0.000 (0.008)	Loss 1.8047 (1.8739)	Prec@1 30.762 (28.724)	Prec@5 83.008 (82.137)	Acc 0.287 (0.231)	
2022-03-31 06:05:31,128 - INFO - TRAINING - Epoch: [0][330/781]	Time 0.178 (0.189)	Data 0.000 (0.008)	Loss 1.5482 (1.8672)	Prec@1 47.656 (28.969)	Prec@5 88.574 (82.342)	Acc 0.290 (0.233)	
