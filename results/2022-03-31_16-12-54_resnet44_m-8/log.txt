2022-03-31 16:12:54,037 - INFO - saving to results/2022-03-31_16-12-54_resnet44_m-8
2022-03-31 16:12:54,037 - DEBUG - run arguments: Namespace(adapt_grad_norm=None, autoaugment=False, batch_size=64, chunk_batch=1, config_file=None, cutmix=None, cutout=True, dataset='cifar10', datasets_dir='~/Datasets', device='cuda', device_ids=[0], dist_backend='nccl', dist_init='env://', distributed=False, drop_optim_state=False, dtype='float', duplicates=8, epochs=100, eval_batch_size=-1, evaluate=None, grad_clip=-1, input_size=None, label_smoothing=0, local_rank=-1, loss_scale=1, lr=0.1, mixup=None, model='resnet', model_config="{'depth': 44}", momentum=0.9, optimizer='SGD', print_freq=10, results_dir='results/', resume='', save='2022-03-31_16-12-54', save_all=False, seed=123, start_epoch=-1, sync_bn=False, tensorwatch=False, tensorwatch_port=0, weight_decay=0, workers=8, world_size=-1)
2022-03-31 16:12:54,037 - INFO - creating model resnet
2022-03-31 16:12:54,083 - INFO - created model with configuration: {'dataset': 'cifar10', 'depth': 44}
2022-03-31 16:12:54,083 - INFO - number of parameters: 661338
2022-03-31 16:12:58,998 - INFO - optimization regime: [{'epoch': 0, 'optimizer': 'SGD', 'lr': 0.1, 'momentum': 0.9, 'regularizer': {'name': 'WeightDecay', 'value': 0.0001, 'log': False, 'filter': {'parameter_name': <function weight_decay_config.<locals>.<lambda> at 0x150d6e927a60>, 'module': <function weight_decay_config.<locals>.<lambda> at 0x150d6e927820>}}}, {'epoch': 81, 'lr': 0.01}, {'epoch': 122, 'lr': 0.001}, {'epoch': 164, 'lr': 0.0001}]
2022-03-31 16:12:58,998 - INFO - data regime: Current: {'datasets_path': '~/Datasets', 'name': 'cifar10', 'split': 'train', 'augment': True, 'input_size': None, 'batch_size': 64, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'drop_last': True, 'distributed': False, 'duplicates': 8, 'autoaugment': False, 'cutout': {'holes': 1, 'length': 16}}
 Regime:None
2022-03-31 16:12:58,998 - INFO - 
Starting Epoch: 1

2022-03-31 16:13:02,352 - DEBUG - OPTIMIZER - setting lr = 0.1
2022-03-31 16:13:02,352 - DEBUG - OPTIMIZER - setting momentum = 0.9
2022-03-31 16:13:03,144 - INFO - TRAINING - Epoch: [0][0/781]	Time 4.145 (4.145)	Data 3.352 (3.352)	Loss 2.3068 (2.3068)	Prec@1 7.031 (7.031)	Prec@5 42.383 (42.383)	Acc 0.070 (0.070)	
2022-03-31 16:13:04,153 - INFO - TRAINING - Epoch: [0][10/781]	Time 0.092 (0.469)	Data 0.000 (0.305)	Loss 2.3304 (2.3035)	Prec@1 11.328 (13.903)	Prec@5 53.516 (55.273)	Acc 0.139 (0.121)	
2022-03-31 16:13:05,150 - INFO - TRAINING - Epoch: [0][20/781]	Time 0.106 (0.293)	Data 0.000 (0.160)	Loss 2.2553 (2.2595)	Prec@1 9.961 (15.597)	Prec@5 63.867 (59.552)	Acc 0.156 (0.137)	
2022-03-31 16:13:06,229 - INFO - TRAINING - Epoch: [0][30/781]	Time 0.092 (0.233)	Data 0.000 (0.108)	Loss 2.1630 (2.2303)	Prec@1 25.781 (17.043)	Prec@5 67.578 (62.374)	Acc 0.170 (0.145)	
2022-03-31 16:13:07,262 - INFO - TRAINING - Epoch: [0][40/781]	Time 0.092 (0.202)	Data 0.000 (0.083)	Loss 2.1939 (2.1895)	Prec@1 22.070 (18.016)	Prec@5 75.000 (65.939)	Acc 0.180 (0.153)	
2022-03-31 16:13:08,233 - INFO - TRAINING - Epoch: [0][50/781]	Time 0.101 (0.181)	Data 0.000 (0.067)	Loss 1.9182 (2.1601)	Prec@1 23.828 (18.865)	Prec@5 79.883 (67.946)	Acc 0.189 (0.159)	
2022-03-31 16:13:09,257 - INFO - TRAINING - Epoch: [0][60/781]	Time 0.094 (0.168)	Data 0.000 (0.056)	Loss 1.9334 (2.1328)	Prec@1 20.312 (19.672)	Prec@5 82.031 (69.778)	Acc 0.197 (0.165)	
2022-03-31 16:13:10,294 - INFO - TRAINING - Epoch: [0][70/781]	Time 0.092 (0.159)	Data 0.000 (0.048)	Loss 2.0436 (2.1189)	Prec@1 24.414 (19.897)	Prec@5 82.422 (71.055)	Acc 0.199 (0.170)	
2022-03-31 16:13:11,354 - INFO - TRAINING - Epoch: [0][80/781]	Time 0.133 (0.153)	Data 0.009 (0.042)	Loss 2.0068 (2.1040)	Prec@1 25.781 (20.443)	Prec@5 72.266 (72.020)	Acc 0.204 (0.173)	
2022-03-31 16:13:12,376 - INFO - TRAINING - Epoch: [0][90/781]	Time 0.114 (0.147)	Data 0.018 (0.038)	Loss 2.0399 (2.0880)	Prec@1 22.461 (21.008)	Prec@5 74.219 (73.010)	Acc 0.210 (0.177)	
2022-03-31 16:13:13,460 - INFO - TRAINING - Epoch: [0][100/781]	Time 0.110 (0.143)	Data 0.000 (0.034)	Loss 1.8970 (2.0709)	Prec@1 28.125 (21.718)	Prec@5 76.758 (73.722)	Acc 0.217 (0.181)	
2022-03-31 16:13:14,502 - INFO - TRAINING - Epoch: [0][110/781]	Time 0.092 (0.140)	Data 0.000 (0.031)	Loss 1.8910 (2.0597)	Prec@1 28.711 (22.144)	Prec@5 80.469 (74.277)	Acc 0.221 (0.184)	
2022-03-31 16:13:15,538 - INFO - TRAINING - Epoch: [0][120/781]	Time 0.094 (0.137)	Data 0.000 (0.029)	Loss 1.8596 (2.0472)	Prec@1 30.469 (22.516)	Prec@5 82.617 (74.840)	Acc 0.225 (0.188)	
2022-03-31 16:13:16,611 - INFO - TRAINING - Epoch: [0][130/781]	Time 0.095 (0.134)	Data 0.000 (0.027)	Loss 1.9315 (2.0337)	Prec@1 29.883 (23.016)	Prec@5 84.180 (75.581)	Acc 0.230 (0.191)	
2022-03-31 16:13:17,651 - INFO - TRAINING - Epoch: [0][140/781]	Time 0.095 (0.132)	Data 0.000 (0.025)	Loss 1.8278 (2.0218)	Prec@1 32.617 (23.406)	Prec@5 84.961 (76.198)	Acc 0.234 (0.194)	
2022-03-31 16:13:18,724 - INFO - TRAINING - Epoch: [0][150/781]	Time 0.105 (0.131)	Data 0.000 (0.023)	Loss 1.8030 (2.0139)	Prec@1 32.617 (23.731)	Prec@5 86.914 (76.731)	Acc 0.237 (0.196)	
2022-03-31 16:13:19,742 - INFO - TRAINING - Epoch: [0][160/781]	Time 0.102 (0.129)	Data 0.000 (0.022)	Loss 1.9911 (2.0050)	Prec@1 25.586 (24.046)	Prec@5 80.273 (77.125)	Acc 0.240 (0.199)	
2022-03-31 16:13:20,769 - INFO - TRAINING - Epoch: [0][170/781]	Time 0.125 (0.127)	Data 0.000 (0.021)	Loss 1.8714 (1.9970)	Prec@1 23.438 (24.327)	Prec@5 82.031 (77.547)	Acc 0.243 (0.202)	
2022-03-31 16:13:21,827 - INFO - TRAINING - Epoch: [0][180/781]	Time 0.110 (0.126)	Data 0.000 (0.020)	Loss 1.7566 (1.9860)	Prec@1 32.422 (24.673)	Prec@5 85.742 (77.986)	Acc 0.247 (0.204)	
2022-03-31 16:13:22,936 - INFO - TRAINING - Epoch: [0][190/781]	Time 0.124 (0.125)	Data 0.000 (0.019)	Loss 1.7890 (1.9770)	Prec@1 35.742 (24.925)	Prec@5 84.766 (78.413)	Acc 0.249 (0.206)	
2022-03-31 16:13:24,014 - INFO - TRAINING - Epoch: [0][200/781]	Time 0.092 (0.124)	Data 0.000 (0.018)	Loss 1.8048 (1.9679)	Prec@1 34.180 (25.068)	Prec@5 84.375 (78.817)	Acc 0.251 (0.209)	
2022-03-31 16:13:25,089 - INFO - TRAINING - Epoch: [0][210/781]	Time 0.092 (0.124)	Data 0.000 (0.017)	Loss 1.6762 (1.9587)	Prec@1 36.133 (25.332)	Prec@5 94.336 (79.234)	Acc 0.253 (0.211)	
2022-03-31 16:13:26,105 - INFO - TRAINING - Epoch: [0][220/781]	Time 0.092 (0.123)	Data 0.000 (0.016)	Loss 1.8870 (1.9517)	Prec@1 34.570 (25.680)	Prec@5 82.617 (79.492)	Acc 0.257 (0.213)	
