2022-03-31 16:04:23,329 - INFO - saving to results/2022-03-31_16-04-23_resnet44_m-8
2022-03-31 16:04:23,330 - DEBUG - run arguments: Namespace(adapt_grad_norm=None, autoaugment=False, batch_size=64, chunk_batch=1, config_file=None, cutmix=None, cutout=True, dataset='cifar10', datasets_dir='~/Datasets', device='cuda', device_ids=[0], dist_backend='nccl', dist_init='env://', distributed=False, drop_optim_state=False, dtype='float', duplicates=8, epochs=100, eval_batch_size=-1, evaluate=None, grad_clip=-1, input_size=None, label_smoothing=0, local_rank=-1, loss_scale=1, lr=0.1, mixup=None, model='resnet', model_config="{'depth': 44}", momentum=0.9, optimizer='SGD', print_freq=10, results_dir='results/', resume='', save='2022-03-31_16-04-23', save_all=False, seed=123, start_epoch=-1, sync_bn=False, tensorwatch=False, tensorwatch_port=0, weight_decay=0, workers=8, world_size=-1)
2022-03-31 16:04:23,331 - INFO - creating model resnet
2022-03-31 16:04:23,373 - INFO - created model with configuration: {'dataset': 'cifar10', 'depth': 44}
2022-03-31 16:04:23,374 - INFO - number of parameters: 661338
2022-03-31 16:04:28,377 - INFO - optimization regime: [{'epoch': 0, 'optimizer': 'SGD', 'lr': 0.1, 'momentum': 0.9, 'regularizer': {'name': 'WeightDecay', 'value': 0.0001, 'log': False, 'filter': {'parameter_name': <function weight_decay_config.<locals>.<lambda> at 0x148950fd6a60>, 'module': <function weight_decay_config.<locals>.<lambda> at 0x148950fd6820>}}}, {'epoch': 81, 'lr': 0.01}, {'epoch': 122, 'lr': 0.001}, {'epoch': 164, 'lr': 0.0001}]
2022-03-31 16:04:28,380 - INFO - data regime: Current: {'datasets_path': '~/Datasets', 'name': 'cifar10', 'split': 'train', 'augment': True, 'input_size': None, 'batch_size': 64, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'drop_last': True, 'distributed': False, 'duplicates': 8, 'autoaugment': False, 'cutout': {'holes': 1, 'length': 16}}
 Regime:None
2022-03-31 16:04:28,380 - INFO - 
Starting Epoch: 1

2022-03-31 16:04:31,948 - DEBUG - OPTIMIZER - setting lr = 0.1
2022-03-31 16:04:31,948 - DEBUG - OPTIMIZER - setting momentum = 0.9
2022-03-31 16:04:32,788 - INFO - TRAINING - Epoch: [0][0/781]	Time 4.407 (4.407)	Data 3.567 (3.567)	Loss 2.3068 (2.3068)	Prec@1 7.031 (7.031)	Prec@5 42.383 (42.383)	Acc 0.070 (0.070)	
2022-03-31 16:04:33,846 - INFO - TRAINING - Epoch: [0][10/781]	Time 0.111 (0.497)	Data 0.000 (0.326)	Loss 2.3304 (2.3035)	Prec@1 11.328 (13.903)	Prec@5 53.516 (55.273)	Acc 0.139 (0.121)	
2022-03-31 16:04:34,945 - INFO - TRAINING - Epoch: [0][20/781]	Time 0.092 (0.313)	Data 0.000 (0.173)	Loss 2.2553 (2.2595)	Prec@1 9.961 (15.597)	Prec@5 63.867 (59.552)	Acc 0.156 (0.137)	
2022-03-31 16:04:36,071 - INFO - TRAINING - Epoch: [0][30/781]	Time 0.092 (0.248)	Data 0.000 (0.117)	Loss 2.1628 (2.2303)	Prec@1 25.781 (17.043)	Prec@5 67.578 (62.374)	Acc 0.170 (0.145)	
2022-03-31 16:04:37,147 - INFO - TRAINING - Epoch: [0][40/781]	Time 0.105 (0.214)	Data 0.000 (0.089)	Loss 2.1929 (2.1896)	Prec@1 22.461 (18.021)	Prec@5 74.609 (65.939)	Acc 0.180 (0.153)	
2022-03-31 16:04:38,231 - INFO - TRAINING - Epoch: [0][50/781]	Time 0.094 (0.193)	Data 0.000 (0.072)	Loss 1.9201 (2.1597)	Prec@1 24.219 (18.892)	Prec@5 80.273 (68.011)	Acc 0.189 (0.159)	
2022-03-31 16:04:39,326 - INFO - TRAINING - Epoch: [0][60/781]	Time 0.104 (0.179)	Data 0.000 (0.060)	Loss 1.9278 (2.1321)	Prec@1 22.461 (19.707)	Prec@5 82.617 (69.832)	Acc 0.197 (0.165)	
2022-03-31 16:04:40,394 - INFO - TRAINING - Epoch: [0][70/781]	Time 0.092 (0.169)	Data 0.000 (0.052)	Loss 2.0468 (2.1182)	Prec@1 23.633 (19.960)	Prec@5 82.422 (71.121)	Acc 0.200 (0.170)	
2022-03-31 16:04:41,450 - INFO - TRAINING - Epoch: [0][80/781]	Time 0.106 (0.161)	Data 0.000 (0.045)	Loss 2.0007 (2.1033)	Prec@1 26.758 (20.595)	Prec@5 71.875 (72.073)	Acc 0.206 (0.174)	
2022-03-31 16:04:42,545 - INFO - TRAINING - Epoch: [0][90/781]	Time 0.100 (0.156)	Data 0.000 (0.041)	Loss 2.0211 (2.0870)	Prec@1 23.633 (21.120)	Prec@5 75.000 (73.062)	Acc 0.211 (0.178)	
2022-03-31 16:04:43,662 - INFO - TRAINING - Epoch: [0][100/781]	Time 0.116 (0.151)	Data 0.000 (0.037)	Loss 1.9292 (2.0705)	Prec@1 27.734 (21.879)	Prec@5 78.320 (73.828)	Acc 0.219 (0.181)	
2022-03-31 16:04:44,764 - INFO - TRAINING - Epoch: [0][110/781]	Time 0.114 (0.148)	Data 0.000 (0.034)	Loss 1.8878 (2.0593)	Prec@1 27.734 (22.463)	Prec@5 80.273 (74.384)	Acc 0.225 (0.185)	
2022-03-31 16:04:45,765 - INFO - TRAINING - Epoch: [0][120/781]	Time 0.092 (0.144)	Data 0.000 (0.031)	Loss 1.8698 (2.0462)	Prec@1 27.734 (22.774)	Prec@5 83.398 (75.045)	Acc 0.228 (0.188)	
2022-03-31 16:04:46,855 - INFO - TRAINING - Epoch: [0][130/781]	Time 0.092 (0.141)	Data 0.000 (0.029)	Loss 1.9268 (2.0321)	Prec@1 28.125 (23.238)	Prec@5 83.398 (75.786)	Acc 0.232 (0.192)	
2022-03-31 16:04:47,949 - INFO - TRAINING - Epoch: [0][140/781]	Time 0.109 (0.139)	Data 0.000 (0.027)	Loss 1.8544 (2.0202)	Prec@1 31.641 (23.622)	Prec@5 85.352 (76.373)	Acc 0.236 (0.195)	
2022-03-31 16:04:48,994 - INFO - TRAINING - Epoch: [0][150/781]	Time 0.095 (0.137)	Data 0.000 (0.025)	Loss 1.8451 (2.0121)	Prec@1 31.055 (23.943)	Prec@5 84.766 (76.877)	Acc 0.239 (0.198)	
2022-03-31 16:04:50,073 - INFO - TRAINING - Epoch: [0][160/781]	Time 0.130 (0.135)	Data 0.008 (0.024)	Loss 1.9209 (2.0023)	Prec@1 29.883 (24.332)	Prec@5 82.227 (77.303)	Acc 0.243 (0.200)	
2022-03-31 16:04:51,121 - INFO - TRAINING - Epoch: [0][170/781]	Time 0.092 (0.133)	Data 0.000 (0.022)	Loss 1.8338 (1.9948)	Prec@1 25.586 (24.604)	Prec@5 81.250 (77.696)	Acc 0.246 (0.203)	
