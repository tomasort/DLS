2022-03-31 06:04:39,422 - INFO - saving to results/2022-03-31_06-04-39_resnet44_m-16
2022-03-31 06:04:39,425 - DEBUG - run arguments: Namespace(adapt_grad_norm=None, autoaugment=False, batch_size=64, chunk_batch=1, config_file=None, cutmix=None, cutout=True, dataset='cifar10', datasets_dir='~/Datasets', device='cuda', device_ids=[0], dist_backend='nccl', dist_init='env://', distributed=False, drop_optim_state=False, dtype='float', duplicates=16, epochs=100, eval_batch_size=-1, evaluate=None, grad_clip=-1, input_size=None, label_smoothing=0, local_rank=-1, loss_scale=1, lr=0.1, mixup=None, model='resnet', model_config="{'depth': 44}", momentum=0.9, optimizer='SGD', print_freq=10, results_dir='results/', resume='', save='2022-03-31_06-04-39', save_all=False, seed=123, start_epoch=-1, sync_bn=False, tensorwatch=False, tensorwatch_port=0, weight_decay=0, workers=8, world_size=-1)
2022-03-31 06:04:39,425 - INFO - creating model resnet
2022-03-31 06:04:39,495 - INFO - created model with configuration: {'dataset': 'cifar10', 'depth': 44}
2022-03-31 06:04:39,499 - INFO - number of parameters: 661338
2022-03-31 06:04:47,207 - INFO - optimization regime: [{'epoch': 0, 'optimizer': 'SGD', 'lr': 0.1, 'momentum': 0.9, 'regularizer': {'name': 'WeightDecay', 'value': 0.0001, 'log': False, 'filter': {'parameter_name': <function weight_decay_config.<locals>.<lambda> at 0x14a41bf35a60>, 'module': <function weight_decay_config.<locals>.<lambda> at 0x14a41bf35820>}}}, {'epoch': 81, 'lr': 0.01}, {'epoch': 122, 'lr': 0.001}, {'epoch': 164, 'lr': 0.0001}]
2022-03-31 06:04:47,213 - INFO - data regime: Current: {'datasets_path': '~/Datasets', 'name': 'cifar10', 'split': 'train', 'augment': True, 'input_size': None, 'batch_size': 64, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'drop_last': True, 'distributed': False, 'duplicates': 16, 'autoaugment': False, 'cutout': {'holes': 1, 'length': 16}}
 Regime:None
2022-03-31 06:04:47,213 - INFO - 
Starting Epoch: 1

2022-03-31 06:04:49,766 - DEBUG - OPTIMIZER - setting lr = 0.1
2022-03-31 06:04:49,776 - DEBUG - OPTIMIZER - setting momentum = 0.9
2022-03-31 06:04:50,610 - INFO - TRAINING - Epoch: [0][0/781]	Time 3.396 (3.396)	Data 2.552 (2.552)	Loss 2.3063 (2.3063)	Prec@1 8.203 (8.203)	Prec@5 41.309 (41.309)	Acc 0.082 (0.082)	
2022-03-31 06:04:52,004 - INFO - TRAINING - Epoch: [0][10/781]	Time 0.179 (0.458)	Data 0.000 (0.235)	Loss 2.3270 (2.3034)	Prec@1 10.938 (13.707)	Prec@5 53.906 (55.513)	Acc 0.137 (0.122)	
2022-03-31 06:04:53,786 - INFO - TRAINING - Epoch: [0][20/781]	Time 0.178 (0.325)	Data 0.000 (0.124)	Loss 2.2528 (2.2592)	Prec@1 10.742 (15.332)	Prec@5 64.844 (59.761)	Acc 0.153 (0.136)	
2022-03-31 06:04:55,569 - INFO - TRAINING - Epoch: [0][30/781]	Time 0.178 (0.277)	Data 0.000 (0.084)	Loss 2.1510 (2.2293)	Prec@1 26.172 (16.797)	Prec@5 67.090 (62.582)	Acc 0.168 (0.144)	
2022-03-31 06:04:57,351 - INFO - TRAINING - Epoch: [0][40/781]	Time 0.178 (0.253)	Data 0.000 (0.064)	Loss 2.1891 (2.1883)	Prec@1 20.215 (17.731)	Prec@5 75.391 (66.120)	Acc 0.177 (0.151)	
2022-03-31 06:04:59,136 - INFO - TRAINING - Epoch: [0][50/781]	Time 0.178 (0.239)	Data 0.000 (0.051)	Loss 1.9107 (2.1584)	Prec@1 28.320 (18.838)	Prec@5 80.273 (68.237)	Acc 0.188 (0.158)	
2022-03-31 06:05:00,919 - INFO - TRAINING - Epoch: [0][60/781]	Time 0.178 (0.229)	Data 0.000 (0.043)	Loss 1.9363 (2.1326)	Prec@1 21.875 (19.552)	Prec@5 83.691 (69.972)	Acc 0.196 (0.163)	
2022-03-31 06:05:02,701 - INFO - TRAINING - Epoch: [0][70/781]	Time 0.178 (0.222)	Data 0.000 (0.037)	Loss 2.0177 (2.1188)	Prec@1 26.074 (19.863)	Prec@5 80.762 (71.153)	Acc 0.199 (0.168)	
2022-03-31 06:05:04,545 - INFO - TRAINING - Epoch: [0][80/781]	Time 0.178 (0.217)	Data 0.000 (0.033)	Loss 2.0018 (2.1035)	Prec@1 25.684 (20.433)	Prec@5 74.609 (72.192)	Acc 0.204 (0.172)	
2022-03-31 06:05:06,328 - INFO - TRAINING - Epoch: [0][90/781]	Time 0.178 (0.213)	Data 0.000 (0.030)	Loss 2.0052 (2.0869)	Prec@1 23.340 (20.989)	Prec@5 76.855 (73.241)	Acc 0.210 (0.176)	
2022-03-31 06:05:08,109 - INFO - TRAINING - Epoch: [0][100/781]	Time 0.178 (0.209)	Data 0.000 (0.027)	Loss 1.9327 (2.0714)	Prec@1 29.492 (21.737)	Prec@5 77.539 (74.032)	Acc 0.217 (0.180)	
2022-03-31 06:05:09,892 - INFO - TRAINING - Epoch: [0][110/781]	Time 0.178 (0.207)	Data 0.000 (0.024)	Loss 1.8497 (2.0582)	Prec@1 30.762 (22.239)	Prec@5 81.250 (74.571)	Acc 0.222 (0.183)	
2022-03-31 06:05:11,675 - INFO - TRAINING - Epoch: [0][120/781]	Time 0.178 (0.204)	Data 0.000 (0.022)	Loss 1.8321 (2.0454)	Prec@1 28.711 (22.617)	Prec@5 84.863 (75.203)	Acc 0.226 (0.187)	
2022-03-31 06:05:13,457 - INFO - TRAINING - Epoch: [0][130/781]	Time 0.178 (0.202)	Data 0.000 (0.021)	Loss 1.8725 (2.0309)	Prec@1 30.371 (23.148)	Prec@5 84.473 (75.850)	Acc 0.231 (0.190)	
2022-03-31 06:05:15,238 - INFO - TRAINING - Epoch: [0][140/781]	Time 0.178 (0.201)	Data 0.000 (0.019)	Loss 1.8666 (2.0204)	Prec@1 28.027 (23.461)	Prec@5 86.133 (76.446)	Acc 0.235 (0.193)	
2022-03-31 06:05:17,020 - INFO - TRAINING - Epoch: [0][150/781]	Time 0.178 (0.199)	Data 0.000 (0.018)	Loss 1.8364 (2.0126)	Prec@1 27.051 (23.724)	Prec@5 85.742 (76.870)	Acc 0.237 (0.196)	
2022-03-31 06:05:18,804 - INFO - TRAINING - Epoch: [0][160/781]	Time 0.178 (0.198)	Data 0.000 (0.017)	Loss 1.9991 (2.0024)	Prec@1 26.465 (24.037)	Prec@5 80.762 (77.344)	Acc 0.240 (0.199)	
2022-03-31 06:05:20,585 - INFO - TRAINING - Epoch: [0][170/781]	Time 0.178 (0.197)	Data 0.000 (0.016)	Loss 1.9062 (1.9948)	Prec@1 24.609 (24.269)	Prec@5 78.809 (77.714)	Acc 0.243 (0.201)	
2022-03-31 06:05:22,367 - INFO - TRAINING - Epoch: [0][180/781]	Time 0.178 (0.196)	Data 0.000 (0.015)	Loss 1.7394 (1.9828)	Prec@1 26.953 (24.679)	Prec@5 85.938 (78.173)	Acc 0.247 (0.204)	
2022-03-31 06:05:24,148 - INFO - TRAINING - Epoch: [0][190/781]	Time 0.178 (0.195)	Data 0.000 (0.014)	Loss 1.7498 (1.9718)	Prec@1 36.426 (25.027)	Prec@5 84.961 (78.636)	Acc 0.250 (0.206)	
2022-03-31 06:05:25,930 - INFO - TRAINING - Epoch: [0][200/781]	Time 0.178 (0.194)	Data 0.000 (0.014)	Loss 1.8123 (1.9624)	Prec@1 30.273 (25.227)	Prec@5 84.180 (79.012)	Acc 0.252 (0.208)	
2022-03-31 06:05:27,710 - INFO - TRAINING - Epoch: [0][210/781]	Time 0.178 (0.193)	Data 0.000 (0.013)	Loss 1.6968 (1.9542)	Prec@1 33.203 (25.494)	Prec@5 91.797 (79.366)	Acc 0.255 (0.210)	
2022-03-31 06:05:29,490 - INFO - TRAINING - Epoch: [0][220/781]	Time 0.178 (0.192)	Data 0.000 (0.012)	Loss 1.9456 (1.9467)	Prec@1 32.812 (25.841)	Prec@5 80.176 (79.674)	Acc 0.258 (0.212)	
2022-03-31 06:05:31,271 - INFO - TRAINING - Epoch: [0][230/781]	Time 0.178 (0.192)	Data 0.000 (0.012)	Loss 1.8278 (1.9386)	Prec@1 29.492 (26.129)	Prec@5 85.352 (80.027)	Acc 0.261 (0.215)	
2022-03-31 06:05:33,051 - INFO - TRAINING - Epoch: [0][240/781]	Time 0.178 (0.191)	Data 0.000 (0.011)	Loss 1.6196 (1.9288)	Prec@1 42.188 (26.543)	Prec@5 89.746 (80.376)	Acc 0.265 (0.217)	
2022-03-31 06:05:34,831 - INFO - TRAINING - Epoch: [0][250/781]	Time 0.178 (0.191)	Data 0.000 (0.011)	Loss 1.6673 (1.9197)	Prec@1 37.012 (26.946)	Prec@5 88.281 (80.701)	Acc 0.269 (0.219)	
2022-03-31 06:05:36,611 - INFO - TRAINING - Epoch: [0][260/781]	Time 0.178 (0.190)	Data 0.000 (0.011)	Loss 1.9171 (1.9140)	Prec@1 30.078 (27.129)	Prec@5 80.957 (80.900)	Acc 0.271 (0.221)	
