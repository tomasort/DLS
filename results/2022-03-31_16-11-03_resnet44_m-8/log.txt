2022-03-31 16:11:03,006 - INFO - saving to results/2022-03-31_16-11-03_resnet44_m-8
2022-03-31 16:11:03,007 - DEBUG - run arguments: Namespace(adapt_grad_norm=None, autoaugment=False, batch_size=64, chunk_batch=1, config_file=None, cutmix=None, cutout=True, dataset='cifar10', datasets_dir='~/Datasets', device='cuda', device_ids=[0], dist_backend='nccl', dist_init='env://', distributed=False, drop_optim_state=False, dtype='float', duplicates=8, epochs=100, eval_batch_size=-1, evaluate=None, grad_clip=-1, input_size=None, label_smoothing=0, local_rank=-1, loss_scale=1, lr=0.1, mixup=None, model='resnet', model_config="{'depth': 44}", momentum=0.9, optimizer='SGD', print_freq=10, results_dir='results/', resume='', save='2022-03-31_16-11-03', save_all=False, seed=123, start_epoch=-1, sync_bn=False, tensorwatch=False, tensorwatch_port=0, weight_decay=0, workers=8, world_size=-1)
2022-03-31 16:11:03,007 - INFO - creating model resnet
2022-03-31 16:11:03,053 - INFO - created model with configuration: {'dataset': 'cifar10', 'depth': 44}
2022-03-31 16:11:03,053 - INFO - number of parameters: 661338
2022-03-31 16:11:07,869 - INFO - optimization regime: [{'epoch': 0, 'optimizer': 'SGD', 'lr': 0.1, 'momentum': 0.9, 'regularizer': {'name': 'WeightDecay', 'value': 0.0001, 'log': False, 'filter': {'parameter_name': <function weight_decay_config.<locals>.<lambda> at 0x14fef1830a60>, 'module': <function weight_decay_config.<locals>.<lambda> at 0x14fef1830820>}}}, {'epoch': 81, 'lr': 0.01}, {'epoch': 122, 'lr': 0.001}, {'epoch': 164, 'lr': 0.0001}]
2022-03-31 16:11:07,869 - INFO - data regime: Current: {'datasets_path': '~/Datasets', 'name': 'cifar10', 'split': 'train', 'augment': True, 'input_size': None, 'batch_size': 64, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'drop_last': True, 'distributed': False, 'duplicates': 8, 'autoaugment': False, 'cutout': {'holes': 1, 'length': 16}}
 Regime:None
2022-03-31 16:11:07,869 - INFO - 
Starting Epoch: 1

2022-03-31 16:11:11,373 - DEBUG - OPTIMIZER - setting lr = 0.1
2022-03-31 16:11:11,374 - DEBUG - OPTIMIZER - setting momentum = 0.9
2022-03-31 16:11:12,110 - INFO - TRAINING - Epoch: [0][0/781]	Time 4.240 (4.240)	Data 3.503 (3.503)	Loss 2.3068 (2.3068)	Prec@1 7.031 (7.031)	Prec@5 42.383 (42.383)	Acc 0.070 (0.070)	
2022-03-31 16:11:13,227 - INFO - TRAINING - Epoch: [0][10/781]	Time 0.127 (0.487)	Data 0.000 (0.320)	Loss 2.3304 (2.3035)	Prec@1 11.328 (13.903)	Prec@5 53.516 (55.273)	Acc 0.139 (0.121)	
2022-03-31 16:11:14,290 - INFO - TRAINING - Epoch: [0][20/781]	Time 0.108 (0.306)	Data 0.000 (0.168)	Loss 2.2553 (2.2595)	Prec@1 9.961 (15.597)	Prec@5 63.867 (59.552)	Acc 0.156 (0.137)	
2022-03-31 16:11:15,325 - INFO - TRAINING - Epoch: [0][30/781]	Time 0.092 (0.241)	Data 0.000 (0.114)	Loss 2.1629 (2.2303)	Prec@1 25.781 (17.043)	Prec@5 67.578 (62.374)	Acc 0.170 (0.145)	
2022-03-31 16:11:16,376 - INFO - TRAINING - Epoch: [0][40/781]	Time 0.092 (0.207)	Data 0.000 (0.087)	Loss 2.1934 (2.1895)	Prec@1 22.266 (18.012)	Prec@5 74.805 (65.939)	Acc 0.180 (0.153)	
2022-03-31 16:11:17,438 - INFO - TRAINING - Epoch: [0][50/781]	Time 0.092 (0.188)	Data 0.000 (0.070)	Loss 1.9189 (2.1599)	Prec@1 23.828 (18.865)	Prec@5 79.688 (67.957)	Acc 0.189 (0.159)	
2022-03-31 16:11:18,473 - INFO - TRAINING - Epoch: [0][60/781]	Time 0.093 (0.174)	Data 0.000 (0.058)	Loss 1.9350 (2.1325)	Prec@1 21.094 (19.659)	Prec@5 82.617 (69.800)	Acc 0.197 (0.165)	
2022-03-31 16:11:19,497 - INFO - TRAINING - Epoch: [0][70/781]	Time 0.101 (0.164)	Data 0.000 (0.050)	Loss 2.0466 (2.1186)	Prec@1 24.023 (19.908)	Prec@5 82.422 (71.066)	Acc 0.199 (0.170)	
2022-03-31 16:11:20,478 - INFO - TRAINING - Epoch: [0][80/781]	Time 0.092 (0.156)	Data 0.000 (0.044)	Loss 1.9978 (2.1035)	Prec@1 26.172 (20.515)	Prec@5 72.461 (72.056)	Acc 0.205 (0.174)	
2022-03-31 16:11:21,525 - INFO - TRAINING - Epoch: [0][90/781]	Time 0.111 (0.150)	Data 0.000 (0.039)	Loss 2.0445 (2.0876)	Prec@1 22.656 (21.053)	Prec@5 74.609 (72.980)	Acc 0.211 (0.177)	
2022-03-31 16:11:22,544 - INFO - TRAINING - Epoch: [0][100/781]	Time 0.092 (0.145)	Data 0.000 (0.036)	Loss 1.9108 (2.0709)	Prec@1 28.516 (21.813)	Prec@5 78.125 (73.760)	Acc 0.218 (0.181)	
2022-03-31 16:11:23,593 - INFO - TRAINING - Epoch: [0][110/781]	Time 0.093 (0.142)	Data 0.000 (0.032)	Loss 1.8787 (2.0584)	Prec@1 27.930 (22.352)	Prec@5 80.078 (74.340)	Acc 0.224 (0.185)	
2022-03-31 16:11:24,562 - INFO - TRAINING - Epoch: [0][120/781]	Time 0.092 (0.138)	Data 0.000 (0.030)	Loss 1.8529 (2.0448)	Prec@1 25.000 (22.677)	Prec@5 83.008 (74.998)	Acc 0.227 (0.188)	
2022-03-31 16:11:25,603 - INFO - TRAINING - Epoch: [0][130/781]	Time 0.092 (0.135)	Data 0.000 (0.028)	Loss 1.9225 (2.0309)	Prec@1 30.273 (23.126)	Prec@5 83.594 (75.725)	Acc 0.231 (0.191)	
2022-03-31 16:11:26,645 - INFO - TRAINING - Epoch: [0][140/781]	Time 0.122 (0.133)	Data 0.000 (0.026)	Loss 1.8468 (2.0188)	Prec@1 30.664 (23.530)	Prec@5 86.719 (76.355)	Acc 0.235 (0.194)	
2022-03-31 16:11:27,704 - INFO - TRAINING - Epoch: [0][150/781]	Time 0.119 (0.131)	Data 0.000 (0.024)	Loss 1.8265 (2.0118)	Prec@1 31.836 (23.784)	Prec@5 85.352 (76.830)	Acc 0.238 (0.197)	
2022-03-31 16:11:28,733 - INFO - TRAINING - Epoch: [0][160/781]	Time 0.126 (0.130)	Data 0.000 (0.022)	Loss 1.9963 (2.0027)	Prec@1 24.023 (24.097)	Prec@5 80.078 (77.262)	Acc 0.241 (0.200)	
2022-03-31 16:11:29,754 - INFO - TRAINING - Epoch: [0][170/781]	Time 0.098 (0.128)	Data 0.000 (0.021)	Loss 1.8467 (1.9944)	Prec@1 26.367 (24.407)	Prec@5 81.055 (77.656)	Acc 0.244 (0.202)	
2022-03-31 16:11:30,769 - INFO - TRAINING - Epoch: [0][180/781]	Time 0.106 (0.127)	Data 0.000 (0.020)	Loss 1.7563 (1.9834)	Prec@1 29.102 (24.813)	Prec@5 87.109 (78.144)	Acc 0.248 (0.205)	
2022-03-31 16:11:31,816 - INFO - TRAINING - Epoch: [0][190/781]	Time 0.096 (0.125)	Data 0.000 (0.019)	Loss 1.8040 (1.9739)	Prec@1 36.523 (25.160)	Prec@5 82.617 (78.558)	Acc 0.252 (0.207)	
2022-03-31 16:11:32,865 - INFO - TRAINING - Epoch: [0][200/781]	Time 0.137 (0.124)	Data 0.000 (0.018)	Loss 1.8030 (1.9653)	Prec@1 31.055 (25.337)	Prec@5 86.328 (78.937)	Acc 0.253 (0.209)	
2022-03-31 16:11:33,915 - INFO - TRAINING - Epoch: [0][210/781]	Time 0.109 (0.123)	Data 0.000 (0.017)	Loss 1.7185 (1.9570)	Prec@1 36.328 (25.598)	Prec@5 90.234 (79.298)	Acc 0.256 (0.211)	
2022-03-31 16:11:34,898 - INFO - TRAINING - Epoch: [0][220/781]	Time 0.091 (0.122)	Data 0.000 (0.016)	Loss 1.9331 (1.9507)	Prec@1 31.641 (25.907)	Prec@5 80.859 (79.549)	Acc 0.259 (0.214)	
2022-03-31 16:11:35,927 - INFO - TRAINING - Epoch: [0][230/781]	Time 0.121 (0.121)	Data 0.000 (0.016)	Loss 1.8193 (1.9447)	Prec@1 33.789 (26.127)	Prec@5 85.938 (79.859)	Acc 0.261 (0.216)	
2022-03-31 16:11:36,958 - INFO - TRAINING - Epoch: [0][240/781]	Time 0.095 (0.121)	Data 0.000 (0.015)	Loss 1.5913 (1.9347)	Prec@1 41.602 (26.558)	Prec@5 91.016 (80.196)	Acc 0.266 (0.218)	
2022-03-31 16:11:37,960 - INFO - TRAINING - Epoch: [0][250/781]	Time 0.092 (0.120)	Data 0.000 (0.015)	Loss 1.6524 (1.9259)	Prec@1 39.453 (26.916)	Prec@5 87.500 (80.493)	Acc 0.269 (0.220)	
2022-03-31 16:11:38,922 - INFO - TRAINING - Epoch: [0][260/781]	Time 0.092 (0.119)	Data 0.000 (0.014)	Loss 1.9922 (1.9216)	Prec@1 30.078 (27.052)	Prec@5 77.344 (80.645)	Acc 0.271 (0.221)	
2022-03-31 16:11:39,936 - INFO - TRAINING - Epoch: [0][270/781]	Time 0.091 (0.118)	Data 0.000 (0.014)	Loss 1.7792 (1.9152)	Prec@1 33.594 (27.282)	Prec@5 84.961 (80.908)	Acc 0.273 (0.223)	
2022-03-31 16:11:40,946 - INFO - TRAINING - Epoch: [0][280/781]	Time 0.102 (0.118)	Data 0.000 (0.013)	Loss 1.5892 (1.9062)	Prec@1 46.094 (27.693)	Prec@5 90.234 (81.153)	Acc 0.277 (0.225)	
