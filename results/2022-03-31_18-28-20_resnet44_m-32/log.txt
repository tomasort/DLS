2022-03-31 18:28:20,644 - INFO - saving to results/2022-03-31_18-28-20_resnet44_m-32
2022-03-31 18:28:20,645 - DEBUG - run arguments: Namespace(adapt_grad_norm=None, autoaugment=False, batch_size=64, chunk_batch=1, config_file=None, cutmix=None, cutout=True, dataset='cifar10', datasets_dir='~/Datasets', device='cuda', device_ids=[0], dist_backend='nccl', dist_init='env://', distributed=False, drop_optim_state=False, dtype='float', duplicates=32, epochs=100, eval_batch_size=-1, evaluate=None, grad_clip=-1, input_size=None, label_smoothing=0, local_rank=-1, loss_scale=1, lr=0.1, mixup=None, model='resnet', model_config="{'depth': 44}", momentum=0.9, optimizer='SGD', print_freq=10, results_dir='results/', resume='', save='2022-03-31_18-28-20', save_all=False, seed=123, start_epoch=-1, sync_bn=False, tensorwatch=False, tensorwatch_port=0, weight_decay=0, workers=8, world_size=-1)
2022-03-31 18:28:20,645 - INFO - creating model resnet
2022-03-31 18:28:20,707 - INFO - created model with configuration: {'dataset': 'cifar10', 'depth': 44}
2022-03-31 18:28:20,708 - INFO - number of parameters: 661338
2022-03-31 18:28:26,463 - INFO - optimization regime: [{'epoch': 0, 'optimizer': 'SGD', 'lr': 0.1, 'momentum': 0.9, 'regularizer': {'name': 'WeightDecay', 'value': 0.0001, 'log': False, 'filter': {'parameter_name': <function weight_decay_config.<locals>.<lambda> at 0x14d5f402da60>, 'module': <function weight_decay_config.<locals>.<lambda> at 0x14d5f402d820>}}}, {'epoch': 81, 'lr': 0.01}, {'epoch': 122, 'lr': 0.001}, {'epoch': 164, 'lr': 0.0001}]
2022-03-31 18:28:26,463 - INFO - data regime: Current: {'datasets_path': '~/Datasets', 'name': 'cifar10', 'split': 'train', 'augment': True, 'input_size': None, 'batch_size': 64, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'drop_last': True, 'distributed': False, 'duplicates': 32, 'autoaugment': False, 'cutout': {'holes': 1, 'length': 16}}
 Regime:None
2022-03-31 18:28:26,463 - INFO - 
Starting Epoch: 1

2022-03-31 18:28:30,028 - DEBUG - OPTIMIZER - setting lr = 0.1
2022-03-31 18:28:30,028 - DEBUG - OPTIMIZER - setting momentum = 0.9
2022-03-31 18:28:31,181 - INFO - TRAINING - Epoch: [0][0/781]	Time 4.717 (4.717)	Data 3.563 (3.563)	Loss 2.3066 (2.3066)	Prec@1 6.738 (6.738)	Prec@5 41.748 (41.748)	Acc 0.067 (0.067)	
2022-03-31 18:28:33,871 - INFO - TRAINING - Epoch: [0][10/781]	Time 0.268 (0.673)	Data 0.000 (0.324)	Loss 2.3284 (2.3035)	Prec@1 10.449 (13.450)	Prec@5 54.395 (55.562)	Acc 0.134 (0.117)	
2022-03-31 18:28:36,551 - INFO - TRAINING - Epoch: [0][20/781]	Time 0.268 (0.480)	Data 0.000 (0.170)	Loss 2.2577 (2.2598)	Prec@1 11.133 (14.983)	Prec@5 63.232 (59.761)	Acc 0.150 (0.131)	
2022-03-31 18:28:39,231 - INFO - TRAINING - Epoch: [0][30/781]	Time 0.268 (0.412)	Data 0.000 (0.115)	Loss 2.1469 (2.2293)	Prec@1 25.488 (16.562)	Prec@5 67.676 (62.596)	Acc 0.166 (0.140)	
2022-03-31 18:28:41,911 - INFO - TRAINING - Epoch: [0][40/781]	Time 0.268 (0.377)	Data 0.000 (0.087)	Loss 2.1913 (2.1881)	Prec@1 20.605 (17.679)	Prec@5 75.635 (66.029)	Acc 0.177 (0.148)	
2022-03-31 18:28:44,593 - INFO - TRAINING - Epoch: [0][50/781]	Time 0.268 (0.355)	Data 0.000 (0.070)	Loss 1.9219 (2.1593)	Prec@1 27.344 (18.755)	Prec@5 81.055 (68.135)	Acc 0.188 (0.155)	
2022-03-31 18:28:47,280 - INFO - TRAINING - Epoch: [0][60/781]	Time 0.268 (0.341)	Data 0.000 (0.059)	Loss 1.9404 (2.1335)	Prec@1 20.117 (19.374)	Prec@5 82.812 (69.938)	Acc 0.194 (0.161)	
2022-03-31 18:28:49,959 - INFO - TRAINING - Epoch: [0][70/781]	Time 0.268 (0.331)	Data 0.000 (0.050)	Loss 2.0331 (2.1192)	Prec@1 25.439 (19.690)	Prec@5 79.980 (71.119)	Acc 0.197 (0.166)	
2022-03-31 18:28:52,638 - INFO - TRAINING - Epoch: [0][80/781]	Time 0.268 (0.323)	Data 0.000 (0.044)	Loss 1.9872 (2.1038)	Prec@1 25.537 (20.226)	Prec@5 76.367 (72.192)	Acc 0.202 (0.170)	
2022-03-31 18:28:55,318 - INFO - TRAINING - Epoch: [0][90/781]	Time 0.268 (0.317)	Data 0.000 (0.039)	Loss 2.0063 (2.0866)	Prec@1 21.924 (20.786)	Prec@5 77.344 (73.280)	Acc 0.208 (0.174)	
2022-03-31 18:28:58,002 - INFO - TRAINING - Epoch: [0][100/781]	Time 0.268 (0.312)	Data 0.000 (0.035)	Loss 1.8963 (2.0701)	Prec@1 28.564 (21.610)	Prec@5 78.369 (74.070)	Acc 0.216 (0.177)	
2022-03-31 18:29:00,682 - INFO - TRAINING - Epoch: [0][110/781]	Time 0.268 (0.308)	Data 0.000 (0.032)	Loss 1.8916 (2.0581)	Prec@1 30.420 (22.191)	Prec@5 80.713 (74.612)	Acc 0.222 (0.181)	
2022-03-31 18:29:03,362 - INFO - TRAINING - Epoch: [0][120/781]	Time 0.268 (0.305)	Data 0.000 (0.030)	Loss 1.8383 (2.0451)	Prec@1 27.295 (22.588)	Prec@5 83.936 (75.247)	Acc 0.226 (0.185)	
2022-03-31 18:29:06,041 - INFO - TRAINING - Epoch: [0][130/781]	Time 0.268 (0.302)	Data 0.000 (0.027)	Loss 1.9269 (2.0316)	Prec@1 27.881 (23.086)	Prec@5 82.617 (75.873)	Acc 0.231 (0.188)	
2022-03-31 18:29:08,720 - INFO - TRAINING - Epoch: [0][140/781]	Time 0.268 (0.300)	Data 0.000 (0.025)	Loss 1.8395 (2.0196)	Prec@1 30.127 (23.511)	Prec@5 85.693 (76.474)	Acc 0.235 (0.191)	
2022-03-31 18:29:11,399 - INFO - TRAINING - Epoch: [0][150/781]	Time 0.268 (0.298)	Data 0.000 (0.024)	Loss 1.8235 (2.0118)	Prec@1 28.955 (23.821)	Prec@5 87.109 (76.891)	Acc 0.238 (0.194)	
2022-03-31 18:29:14,082 - INFO - TRAINING - Epoch: [0][160/781]	Time 0.268 (0.296)	Data 0.000 (0.022)	Loss 1.9770 (2.0019)	Prec@1 26.172 (24.082)	Prec@5 81.299 (77.331)	Acc 0.241 (0.197)	
2022-03-31 18:29:16,762 - INFO - TRAINING - Epoch: [0][170/781]	Time 0.268 (0.294)	Data 0.000 (0.021)	Loss 1.8477 (1.9942)	Prec@1 24.170 (24.323)	Prec@5 81.885 (77.729)	Acc 0.243 (0.200)	
2022-03-31 18:29:19,441 - INFO - TRAINING - Epoch: [0][180/781]	Time 0.268 (0.293)	Data 0.000 (0.020)	Loss 1.7672 (1.9831)	Prec@1 26.123 (24.710)	Prec@5 86.035 (78.140)	Acc 0.247 (0.202)	
2022-03-31 18:29:22,121 - INFO - TRAINING - Epoch: [0][190/781]	Time 0.268 (0.291)	Data 0.000 (0.019)	Loss 1.7663 (1.9731)	Prec@1 37.793 (25.045)	Prec@5 85.449 (78.589)	Acc 0.250 (0.205)	
2022-03-31 18:29:24,801 - INFO - TRAINING - Epoch: [0][200/781]	Time 0.268 (0.290)	Data 0.000 (0.018)	Loss 1.8415 (1.9639)	Prec@1 31.006 (25.240)	Prec@5 83.984 (78.990)	Acc 0.252 (0.207)	
2022-03-31 18:29:27,484 - INFO - TRAINING - Epoch: [0][210/781]	Time 0.268 (0.289)	Data 0.000 (0.017)	Loss 1.6625 (1.9547)	Prec@1 36.426 (25.556)	Prec@5 92.529 (79.373)	Acc 0.256 (0.209)	
2022-03-31 18:29:30,163 - INFO - TRAINING - Epoch: [0][220/781]	Time 0.268 (0.288)	Data 0.000 (0.016)	Loss 1.8942 (1.9476)	Prec@1 30.615 (25.871)	Prec@5 83.789 (79.660)	Acc 0.259 (0.212)	
2022-03-31 18:29:32,842 - INFO - TRAINING - Epoch: [0][230/781]	Time 0.268 (0.287)	Data 0.000 (0.016)	Loss 1.8059 (1.9407)	Prec@1 30.566 (26.121)	Prec@5 86.084 (79.969)	Acc 0.261 (0.214)	
2022-03-31 18:29:35,521 - INFO - TRAINING - Epoch: [0][240/781]	Time 0.268 (0.287)	Data 0.000 (0.015)	Loss 1.5990 (1.9307)	Prec@1 42.969 (26.528)	Prec@5 91.211 (80.320)	Acc 0.265 (0.216)	
2022-03-31 18:29:38,200 - INFO - TRAINING - Epoch: [0][250/781]	Time 0.268 (0.286)	Data 0.000 (0.014)	Loss 1.6703 (1.9218)	Prec@1 35.547 (26.851)	Prec@5 89.209 (80.645)	Acc 0.269 (0.218)	
2022-03-31 18:29:40,880 - INFO - TRAINING - Epoch: [0][260/781]	Time 0.268 (0.285)	Data 0.000 (0.014)	Loss 1.9178 (1.9165)	Prec@1 30.176 (27.014)	Prec@5 80.859 (80.835)	Acc 0.270 (0.220)	
2022-03-31 18:29:43,559 - INFO - TRAINING - Epoch: [0][270/781]	Time 0.268 (0.284)	Data 0.000 (0.013)	Loss 1.7693 (1.9098)	Prec@1 34.912 (27.267)	Prec@5 84.375 (81.074)	Acc 0.273 (0.222)	
2022-03-31 18:29:46,238 - INFO - TRAINING - Epoch: [0][280/781]	Time 0.268 (0.284)	Data 0.000 (0.013)	Loss 1.6040 (1.9006)	Prec@1 42.529 (27.616)	Prec@5 88.379 (81.335)	Acc 0.276 (0.223)	
2022-03-31 18:29:48,916 - INFO - TRAINING - Epoch: [0][290/781]	Time 0.268 (0.283)	Data 0.000 (0.012)	Loss 1.8414 (1.8938)	Prec@1 27.881 (27.880)	Prec@5 86.230 (81.590)	Acc 0.279 (0.225)	
2022-03-31 18:29:51,593 - INFO - TRAINING - Epoch: [0][300/781]	Time 0.268 (0.283)	Data 0.000 (0.012)	Loss 1.4540 (1.8853)	Prec@1 47.607 (28.181)	Prec@5 92.480 (81.831)	Acc 0.282 (0.227)	
2022-03-31 18:29:54,271 - INFO - TRAINING - Epoch: [0][310/781]	Time 0.268 (0.282)	Data 0.000 (0.012)	Loss 1.5780 (1.8777)	Prec@1 38.477 (28.496)	Prec@5 93.115 (82.039)	Acc 0.285 (0.229)	
2022-03-31 18:29:56,949 - INFO - TRAINING - Epoch: [0][320/781]	Time 0.268 (0.282)	Data 0.000 (0.011)	Loss 1.7437 (1.8720)	Prec@1 30.957 (28.719)	Prec@5 83.398 (82.203)	Acc 0.287 (0.231)	
2022-03-31 18:29:59,626 - INFO - TRAINING - Epoch: [0][330/781]	Time 0.267 (0.281)	Data 0.000 (0.011)	Loss 1.5262 (1.8650)	Prec@1 45.752 (28.980)	Prec@5 88.818 (82.404)	Acc 0.290 (0.233)	
2022-03-31 18:30:02,304 - INFO - TRAINING - Epoch: [0][340/781]	Time 0.268 (0.281)	Data 0.000 (0.011)	Loss 1.7620 (1.8579)	Prec@1 34.912 (29.311)	Prec@5 89.404 (82.599)	Acc 0.293 (0.234)	
2022-03-31 18:30:04,981 - INFO - TRAINING - Epoch: [0][350/781]	Time 0.268 (0.281)	Data 0.000 (0.010)	Loss 1.7324 (1.8512)	Prec@1 33.203 (29.610)	Prec@5 87.451 (82.772)	Acc 0.296 (0.236)	
2022-03-31 18:30:07,658 - INFO - TRAINING - Epoch: [0][360/781]	Time 0.268 (0.280)	Data 0.000 (0.010)	Loss 1.4577 (1.8437)	Prec@1 44.141 (29.931)	Prec@5 93.896 (83.002)	Acc 0.299 (0.238)	
2022-03-31 18:30:10,335 - INFO - TRAINING - Epoch: [0][370/781]	Time 0.268 (0.280)	Data 0.000 (0.010)	Loss 1.5348 (1.8371)	Prec@1 43.066 (30.220)	Prec@5 89.307 (83.182)	Acc 0.302 (0.239)	
2022-03-31 18:30:13,013 - INFO - TRAINING - Epoch: [0][380/781]	Time 0.268 (0.280)	Data 0.000 (0.010)	Loss 1.4246 (1.8299)	Prec@1 47.656 (30.518)	Prec@5 95.947 (83.376)	Acc 0.305 (0.241)	
2022-03-31 18:30:15,691 - INFO - TRAINING - Epoch: [0][390/781]	Time 0.268 (0.279)	Data 0.000 (0.009)	Loss 1.5699 (1.8235)	Prec@1 44.531 (30.778)	Prec@5 90.283 (83.556)	Acc 0.308 (0.243)	
